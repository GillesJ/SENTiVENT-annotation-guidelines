Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Lefever2008,
abstract = {Breaking news on economic events such as stock splits or mergers and acquisitions has been shown to have a substantial impact on the financial markets. As it is important to be able to automatically identify events in news items accurately and in a timely manner, we present in this paper proof-of-concept experiments for a supervised machine learning approach to economic event detection in newswire text. For this purpose, we created a corpus of Dutch financial news articles in which 10 types of company-specific economic events were annotated. We trained classifiers using various lexical, syntactic and semantic features. We obtain good results based on a basic set of shallow features, thus showing that this method is a viable approach for economic event detection in news text.},
author = {Lefever, Els},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lefever - 2008 - A Classification-based Approach to Economic Event Detection in Dutch News Text.pdf:pdf},
isbn = {9782951740891},
pages = {330--335},
title = {{A Classification-based Approach to Economic Event Detection in Dutch News Text}},
year = {2008}
}
@article{Segers2015,
author = {Segers, Roxane and Vossen, Piek and Rospocher, Marco and Serafini, Luciano and Laparra, Egoitz},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Segers et al. - 2015 - ESO Documentation ( draft ).pdf:pdf},
title = {{ESO Documentation ( draft )}},
year = {2015}
}
@article{Lu2015,
author = {Lu, Jing and Ng, Vincent},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lu, Ng - 2015 - UTD ' s Event Nugget Detection and Coreference System at KBP 2015.pdf:pdf},
number = {1},
title = {{UTD ' s Event Nugget Detection and Coreference System at KBP 2015}},
year = {2015}
}
@article{Segers2016a,
abstract = {This paper presents the Event and Implied Situation Ontology (ESO), a resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology reuses and maps across existing resources such as WordNet, SUMO, VerbNet, Prop-Bank and FrameNet. We describe how ESO is injected into a new version of the Predicate Matrix and illustrate how these resources are used to detect information in large document collections that otherwise would have remained implicit. The model targets interpretations of situations rather than the semantics of verbs per se. The event is interpreted as a situation using RDF taking all event components into account. Hence, the ontology and the linked resources need to be considered from the perspective of this interpretation model.},
author = {Segers, R. and Laparra, Egoitz and Rospocher, M. and Vossen, Piek and Rigau, G. and Ilievski, Filip},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Segers et al. - 2016 - The predicate matrix and the event and implied situation ontology Making more of events.pdf:pdf},
isbn = {9789730207286},
journal = {Proceedings of the 8th Global WordNet Conference, GWC 2016},
title = {{The predicate matrix and the event and implied situation ontology: Making more of events}},
year = {2016}
}
@article{Collobert2011a,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Rambow2016,
author = {Rambow, Owen and Bauer, Daniel and Cardie, Claire and Arrigo, Michael and Diab, Mona},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Rambow et al. - 2016 - The 2016 TAC KBP BeSt Evaluation.pdf:pdf},
title = {{The 2016 TAC KBP BeSt Evaluation}},
year = {2016}
}
@article{Tanev2009,
abstract = {Background DNA flow cytometry describes the use of flow cytometry for estimation of DNA quantity in cell nuclei. The method involves preparation of aqueous suspensions of intact nuclei whose DNA is stained using a DNA fluorochrome. The nuclei are classified according to their relative fluorescence intensity or DNA content. Because the sample preparation and analysis is convenient and rapid, DNA flow cytometry has become a popular method for ploidy screening, detection of mixoploidy and aneuploidy, cell cycle analysis, assessment of the degree of polysomaty, determination of reproductive pathway, and estimation of absolute DNA amount or genome size. While the former applications are relatively straightforward, estimation of absolute DNA amount requires special attention to possible errors in sample preparation and analysis. Scope The article reviews current procedures for estimation of absolute DNA amounts in plants using flow cytometry, with special emphasis on preparation of nuclei suspensions, stoichiometric DNA staining and the use of DNA reference standards. In addition, methodological pitfalls encountered in estimation of intraspecific variation in genome size are discussed as well as problems linked to the use of DNA flow cytometry for fieldwork. Conclusions Reliable estimation of absolute DNA amounts in plants using flow cytometry is not a trivial task. Although several well-proven protocols are available and some factors controlling the precision and reproducibility have been identified, several problems persist: (1) the need for fresh tissues complicates the transfer of samples from field to the laboratory and/or their storage; (2) the role of cytosolic compounds interfering with quantitative DNA staining is not well understood; and (3) the use of a set of internationally agreed DNA reference standards still remains an unrealized goal.},
author = {Tanev, Hristo and Linge, Jens and Zavarella, Vanni and Atkinson, Martin and Steinberger, Ralf},
doi = {10.1093/aob/mci005},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Tanev et al. - 2009 - Exploiting Machine Learning Techniques to Build an Event Extraction System for Portuguese and Spanish.pdf:pdf},
issn = {16470818},
journal = {Machine Learning},
number = {December 2009},
pages = {55--66},
title = {{Exploiting Machine Learning Techniques to Build an Event Extraction System for Portuguese and Spanish}},
year = {2009}
}
@article{Ji2016,
abstract = {In this paper we give an overview of the Tri-lingual Entity Discovery and Linking (EDL) task at the Knowledge Base Population (KBP) track at TAC2016. We will summarize several new and effective research directions including cross-lingual knowledge transfer and exploiting language-specific resources. In this year we make the EDL task as part of end-to-end Tri-lingual cold-start knowledge base construction on a very large document collection (90,003 documents). So we will also measure and investigate the impact of EDL on cold-start slot filling.},
author = {Ji, Heng and Nothman, Joel},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji, Nothman - 2016 - Overview of TAC-KBP2016 Tri-lingual EDL and Its Impact on End-to-End Cold-Start KBP.pdf:pdf},
journal = {Proceedings of the Ninth Text Analysis Conference (TAC 2016)},
title = {{Overview of TAC-KBP2016 Tri-lingual EDL and Its Impact on End-to-End Cold-Start KBP}},
url = {https://tac.nist.gov/publications/2016/additional.papers/TAC2016.KBP{\_}Entity{\_}Discovery{\_}and{\_}Linking{\_}overview.proceedings.pdf},
year = {2016}
}
@article{Judea2016,
abstract = {Event extraction is a difficult information extraction task. Li et al. (2014) explore the benefits of modeling event extraction and two related tasks, entity mention and relation extraction, jointly. This joint system achieves state-of-the-art performance in all tasks. However, as a system oper-ating only at the sentence level, it misses valuable information from other parts of the document. In this paper, we present an incremental approach to make the global context of the entire docu-ment available to the intra-sentential, state-of-the-art event extractor. We show that our method robustly increases performance on two datasets, namely ACE 2005 and TAC 2015.},
author = {Judea, Alex and Strube, Michael},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Judea, Strube - 2016 - Incremental Global Event Extraction.pdf:pdf},
journal = {Coling},
pages = {2279--2289},
title = {{Incremental Global Event Extraction}},
url = {http://www.aclweb.org/anthology/C16-1215.pdf},
year = {2016}
}
@article{Fokkens2013,
abstract = {Repeating experiments is an important in- strument in the scientific toolbox to vali- date previous work and build upon exist- ing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproduc- ing results is still difficult. We show that the deviation that can be found in repro- duction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper under- standing of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.},
author = {Fokkens, Antske and van Erp, Marieke and Postma, Marten and Pedersen, Ted and Vossen, Piek and Freire, Nuno},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Fokkens et al. - 2013 - Offspring from Reproduction Problems What Replication Failure Teaches Us.pdf:pdf},
isbn = {9781937284503},
journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
pages = {1691--1701},
title = {{Offspring from Reproduction Problems : What Replication Failure Teaches Us}},
year = {2013}
}
@misc{Styler2014,
author = {Styler, W. and Crooks, K. and O'Gorman, Tim and Hamang, Mariah},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Styler et al. - 2014 - Richer Event Description (RED) Annotation Guidelines.pdf:pdf},
pages = {1--62},
publisher = {University of Colorado at Boulder},
title = {{Richer Event Description (RED) Annotation Guidelines}},
year = {2014}
}
@article{Postma,
abstract = {We describe Open Dutch WordNet, which has been derived from the Cornetto database, the Princeton WordNet and open source resources. We exploited existing equivalence relations between Cornetto synsets and WordNet synsets in order to move the open source content from Cornetto into WordNet synsets. Currently, Open Dutch Wordnet contains 117,914 synsets, of which 51,588 synsets contain at least one Dutch synonym, which leaves 66,326 synsets still to obtain a Dutch synonym. The average polysemy is 1.5. The resource is currently delivered in XML under the CC BY-SA 4.0 license1 and it has been linked to the Global Wordnet Grid. In order to use the resource, we refer to: https: //github.com/MartenPostma/ OpenDutchWordnet.},
author = {Postma, Marten and van Miltenburg, Emiel and Segers, Roxane and Schoen, Anneleen and Vossen, Piek},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Postma et al. - 2016 - Open Dutch WordNet.pdf:pdf},
isbn = {9789730207286},
journal = {Proceedings of the Eight Global Wordnet Conference},
pages = {300 -- 307},
title = {{Open Dutch WordNet}},
url = {http://wordpress.let.vupr.nl/odwn/},
year = {2016}
}
@article{Ji2014a,
abstract = {In this paper we give an overview of the Entity Discovery and Linking tasks at the Knowledge Base Population track at TAC 2014. In this year we introduced a new end-to-end English entity discovery and linking task which requires a system to take raw texts as input, automatically extract entity mentions, link them to a knowledge base, and cluster NIL mentions. In this paper we provide an overview of the task definition, annotation issues, successful methods and research challenges associated with this new task. This new task has attracted a lot of participants and has intrigued many interesting research problems and potential approaches. We believe it's a promising task to be extended to a tri-lingual setting in KBP2015. 1},
author = {Ji, Heng and Nothman, Joel and Hachey, Ben},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji, Nothman, Hachey - 2014 - Overview of TAC-KBP2014 Entity Discovery and Linking Tasks.pdf:pdf},
journal = {TAC (Text Analysis Conference) 2014},
title = {{Overview of TAC-KBP2014 Entity Discovery and Linking Tasks}},
year = {2014}
}
@inproceedings{Doddington2004,
abstract = {The objective of the ACE program is to develop technology to automatically infer from human language data the entities being mentioned, the relations among these entities that are directly expressed, and the events in which these entities participate. Data sources include audio and image data in addition to pure text, and Arabic and Chinese in addition to English. The effort involves defining the research tasks in detail, collecting and annotating data needed for training, development, and evaluation, and supporting the research with evaluation tools and research workshops. This program began with a pilot study in 1999. The next evaluation is scheduled for September 2004.},
author = {Doddington, George and Mitchell, Alexis and Przybocki, Mark and Ramshaw, Lance and Strassel, Stephanie and Weischedel, Ralph},
booktitle = {Proceedings of LREC},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Doddington et al. - 2004 - The Automatic Content Extraction (ACE) Program Tasks, Data, and Evaluation.pdf:pdf},
title = {{The Automatic Content Extraction (ACE) Program Tasks, Data, and Evaluation}},
year = {2004}
}
@article{Patwardhan2007,
abstract = {We present an information extraction system that decouples the tasks of ﬁnding relevant regions of text and applying extraction pat- terns. We create a self-trained relevant sen- tence classiﬁer to identify relevant regions, and use a semantic afﬁnity measure to au- tomatically learn domain-relevant extraction patterns. We then distinguish primary pat- terns from secondary patterns and apply the patterns selectively in the relevant regions. The resulting IE system achieves good per- formance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and ir- relevant documents for training.},
author = {Patwardhan, Siddharth and Riloff, Ellen},
doi = {10.1145/1352135.1352287},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Patwardhan, Riloff - 2007 - Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.pdf:pdf},
isbn = {9781595937995},
issn = {00978418},
journal = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {June},
pages = {717--727},
title = {{Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions}},
volume = {7},
year = {2007}
}
@article{Kolya2011,
abstract = {This paper, we propose an approach for event extrac-tion and corresponding event actor identification-within the TimeML framework. Firstly, for event ex-traction, we develop SVM based hybrid approach and for event actor identification the baseline model is developed based on the subject information of the dependency-parsed event sentences. Then we develop an unsupervised syntax based model that is based on the relationship of the event verbs with their argument structure extracted from the head information of the chunks in the parsed sentences. Evaluation on a col-lection of TempEval-2 corpus shows the precision, recall and F-measure values for the baseline model as 64.31{\%}, 67.74{\%} and 65.98{\%}, respectively and the syntax based model as 69.12{\%}, 66.90{\%} and 67.99{\%}, respectively.},
author = {Kolya, Anup Kumar and Ekbal, Asif and Bandyopadhyay, Sivaji},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Kolya, Ekbal, Bandyopadhyay - 2011 - A Hybrid Approach for Event Extraction and Event Actor Identification.pdf:pdf},
issn = {13138502},
journal = {International Conference on Recent Advances in Natural Language Processing 2011 (RANLP 2011)},
number = {September},
pages = {592--597},
title = {{A Hybrid Approach for Event Extraction and Event Actor Identification}},
year = {2011}
}
@misc{DeClercq2014,
abstract = {News articles often reflect an opinion or point of view, withcertain topics evoking more diverse opinions than others. For analyzing and better understanding public discourses, identifying such contested topics constitutes an interesting research question. In this paper, we describe an approach that combines NLP techniques and background knowledge from DBpedia for finding disputed topics in news sites. To identify these topics, we annotate each article with DBpedia concepts, extract their categories, and compute a sentiment score in order to identify those categories revealing significant deviations in polarity across different media. We illustrate our approach in a qualitative evaluation on a sample of six popular British and American news sites.},
author = {{De Clercq}, Orph{\'{e}}e and Hertling, Sven and Hoste, Veronique and Ponzetto, Simone Paolo and Paulheim, Heiko},
isbn = {1613-0073},
language = {eng},
publisher = {CEUR 2014},
title = {{Identifying disputed topics in the news}},
url = {http://lib.ugent.be/catalog/pug01:5673909},
year = {2014}
}
@article{Schuster1997,
abstract = {— In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schuster, Mike and Paliwal, Kuldip K.},
doi = {10.1109/78.650093},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Schuster, Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Recurrent neural networks},
number = {11},
pages = {2673--2681},
pmid = {25246403},
title = {{Bidirectional recurrent neural networks}},
volume = {45},
year = {1997}
}
@book{Hogenboom2014,
abstract = {Today's financial markets are inextricably linked with financial events like acquisitions, profit announcements, or product launches. Information extracted from news messages that report on such events could hence be beneficial for financial decision making. The ubiquity of news, however, makes manual analysis impossible, and due to the unstructured nature of text, the (semi-)automatic extraction and application of financial events remains a non-trivial task. Therefore, the studies composing this dissertation investigate 1) how to accurately identify financial events in news text, and 2) how to effectively use such extracted events in financial applications.

Based on a detailed evaluation of current event extraction systems, this thesis presents a competitive, knowledge-driven, semi-automatic system for financial event extraction from text. A novel pattern language, which makes clever use of the system's underlying knowledge base, allows for the definition of simple, yet expressive event extraction rules that can be applied to natural language texts. The system's knowledge-driven internals remain synchronized with the latest market developments through the accompanying event-triggered update language for knowledge bases, enabling the definition of update rules.

Additional research covered by this dissertation investigates the practical applicability of extracted events. In automated stock trading experiments, the best performing trading rules do not only make use of traditional numerical signals, but also employ news-based event signals. Moreover, when cleaning stock data from disruptions caused by financial events, financial risk analyses yield more accurate results. These results suggest that events detected in news can be used advantageously as supplementary parameters in financial applications.},
author = {Hogenboom, Frederik},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Hogenboom - 2014 - Automated Detection of Financial Events in News Text.pdf:pdf},
isbn = {978-90-5892-386-8},
title = {{Automated Detection of Financial Events in News Text}},
url = {http://repub.eur.nl/pub/77237},
year = {2014}
}
@article{Clark2016,
abstract = {A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.},
archivePrefix = {arXiv},
arxivId = {1606.01323},
author = {Clark, Kevin and Manning, Christopher D.},
doi = {10.18653/v1/P16-1061},
eprint = {1606.01323},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Clark, Manning - 2016 - Improving Coreference Resolution by Learning Entity-Level Distributed Representations.pdf:pdf},
isbn = {9781510827585},
title = {{Improving Coreference Resolution by Learning Entity-Level Distributed Representations}},
url = {http://arxiv.org/abs/1606.01323},
year = {2016}
}
@article{Yangarber2000.automatic_acquisition,
author = {Yangarber, Roman and Grishman, Ralph and Tapanainen, Pasi and Huttunen, Silja},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yangarber et al. - 2000 - Automatic Acquisition of Domain Knowledge for Information Extraction.pdf:pdf},
journal = {Proceedings of the 18th ICCL},
pages = {940--946},
title = {{Automatic Acquisition of Domain Knowledge for Information Extraction}},
volume = {2},
year = {2000}
}
@misc{www:TAC_KBP_events_2017,
author = {{Text Analysis Conference}},
title = {{TAC KBP 2017 Event Track}},
url = {https://tac.nist.gov/2017/KBP/Event/index.html},
year = {2017}
}
@article{Li2014.cinuosm,
abstract = {In this paper, we propose a new frame-work that unifies the output of three infor-mation extraction (IE) tasks -entity men-tions, relations and events as an informa-tion network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many rela-tions can now be considered as the re-sultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and sig-nificantly advances state-of-the-art end-to-end event argument extraction.},
author = {Li, Qi and Ji, Heng and Hong, Yu and Li, Sujian},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li et al. - 2014 - Constructing information networks using one single model.pdf:pdf},
journal = {Methods on Natural Language Processing (EMNLP2014)},
pages = {1846--1851},
title = {{Constructing information networks using one single model}},
url = {http://www.aclweb.org/anthology/D/D14/D14-1198.pdf},
year = {2014}
}
@book{Manning:2008:IIR:1394399,
address = {New York, NY, USA},
author = {Manning, Christopher D and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
isbn = {0521865719, 9780521865715},
publisher = {Cambridge University Press},
title = {{Introduction to Information Retrieval}},
url = {https://nlp.stanford.edu/IR-book/},
year = {2008}
}
@article{Zhou2008,
author = {Zhou, Deyu and Zhang, Xuan and He, Yulan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Zhou, Zhang, He - 2008 - Event extraction from Twitter using Non-Parametric Bayesian Mixture Model with Word Embeddings.pdf:pdf},
pages = {808--817},
title = {{Event extraction from Twitter using Non-Parametric Bayesian Mixture Model with Word Embeddings}},
volume = {1},
year = {2008}
}
@article{OGorman2016,
abstract = {There have been a wide range of recent an-notated corpora concerning events, either re-garding event coreference, the temporal order of events, hierarchical " subevent " structure of events, or causal relationships between events. However, although some believe that these different phenomena will display rich inter-actions, relatively few corpora annotate all of those layers of annotation in a unified fashion. This paper describes the annotation method-ology for the Richer Event Descriptions cor-pus, which annotates entities, events, times, their coreference and partial coreference rela-tions, and the temporal, causal and subevent relationships between the events. It suggests that such rich annotations of within-document event phenomena can be built with high qual-ity through a multi-stage annotation pipeline, and that the resultant corpus could be useful for systems hoping to transition from the de-tection of isolated mentions of events toward a richer understanding of events grounded in the temporal, causal, referential and bridging relations that define them.},
author = {O'Gorman, Tim and Wright-Bettner, Kristin and Palmer, Martha},
doi = {10.18653/v1/W16-5706},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/O'Gorman, Wright-Bettner, Palmer - 2016 - Richer Event Description Integrating event coreference with temporal, causal and bridging anno.pdf:pdf},
journal = {Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016)},
pages = {47--56},
title = {{Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation}},
url = {http://aclweb.org/anthology/W16-5706},
year = {2016}
}
@inproceedings{Ellisa,
abstract = {Knowledge Base Population (KBP) is an evaluation track of the Text Analysis Conference (TAC), a workshop series organized by the National Institute of Standards and Technology (NIST). In 2016, TAC KBP's eighth year of operation, the evaluations focused on five tracks targeting information extraction and question answering technologies: Entity Discovery {\&} Linking, Cold Start, Event Arguments, Event Nuggets, and Belief and Sentiment. Linguistic Data Consortium (LDC) at the University of Pennsylvania has supported TAC KBP since 2009, developing, maintaining, and distributing new and existing linguistic resources for the evaluation series, including queries, human-generated responses, assessments, and tools and specifications. This paper describes LDC's resource creation efforts and their results in support of TAC KBP 2016, focusing on changes that were made to meet new requirements.},
author = {Ellis, Joe and Getman, Jeremy and Fore, Dana and Kuster, Neil and Song, Zhiyi and Bies, Ann and Strassel, Stephanie},
booktitle = {Proceedings of TAC KBP 2015 Workshop, National Institute of Standards and Technology, Maryland, USA},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ellis et al. - 2015 - Overview of Linguistic Resources for the TAC KBP 2015 Evaluations Methodologies and Results(3).pdf:pdf},
title = {{Overview of Linguistic Resources for the TAC KBP 2015 Evaluations : Methodologies and Results}},
year = {2015}
}
@article{Ji2011,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking -- linking names in context to entities in the KB -- and Slot Filling -- adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes ("slots") derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.},
author = {Ji, Heng and Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji, Grishman - 2011 - Knowledge Base Population Successful Approaches and Challenges.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Acl},
pages = {1148--1158},
title = {{Knowledge Base Population : Successful Approaches and Challenges}},
year = {2011}
}
@article{Lazic2015,
abstract = {We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 10{\^{}}7 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets.},
author = {Lazic, Nevena and Subramanya, Amarnag and Ringgaard, Michael and Pereira, Fernando},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lazic et al. - 2015 - Plato A Selective Context Model for Entity Resolution.pdf:pdf},
issn = {2307-387X},
journal = {Acl2016},
pages = {503--515},
title = {{Plato: A Selective Context Model for Entity Resolution}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637},
volume = {3},
year = {2015}
}
@article{Chen2015,
abstract = {Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propaga- tion and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.},
author = {Chen, Yubo and Xu, Liheng and Liu, Kang and Zeng, Daojian and Zhao, Jun},
doi = {10.3115/v1/P15-1017},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chen et al. - 2015 - Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
keywords = {event extraction},
pages = {167--176},
title = {{Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks}},
year = {2015}
}
@article{Luo2015,
abstract = {Event detection aims to extract events with specific types from unstructured data, which is the crucial and challenging task in event re-lated applications, such as event coreference resolution and event argument extraction. In this paper, we propose an event detection sys-tem that combines traditional feature-based methods and novel neural network (NN) mod-els. Experiments show that our ensemble ap-proaches can achieve promising performance in the Event Nugget Detection task.},
author = {Luo, Bingfeng and Yang, Honghui and Zeng, Ying and Feng, Yansong and Zhao, Dongyan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Luo et al. - 2015 - WIP Event Detection System in TAC KBP 2015 Event Nugget Track.pdf:pdf},
journal = {Proceedings of Text Analysis Conference},
title = {{WIP Event Detection System in TAC KBP 2015 Event Nugget Track}},
url = {https://tac.nist.gov/publications/2016/participant.papers/TAC2016.wip.proceedings.pdf},
year = {2015}
}
@article{Ma2016,
abstract = {Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44{\%} F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01{\%}.},
archivePrefix = {arXiv},
arxivId = {1603.04553},
author = {Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard},
eprint = {1603.04553},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ma, Liu, Hovy - 2016 - Unsupervised Ranking Model for Entity Coreference Resolution.pdf:pdf},
isbn = {9781941643914},
pages = {1012--1018},
title = {{Unsupervised Ranking Model for Entity Coreference Resolution}},
url = {http://arxiv.org/abs/1603.04553},
year = {2016}
}
@article{Li2010,
author = {Li, Hao and Li, Xiang and Ji, Heng and Marton, Yuval},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li et al. - 2010 - Domain-Independent Novel Event Discovery and Semi-Automatic Event Annotation.pdf:pdf},
isbn = {9784905166009},
journal = {Paclic},
keywords = {domain-independent,information extraction,novel event discovery,semantic role labeling},
number = {Section 3},
pages = {233--242},
title = {{Domain-Independent Novel Event Discovery and Semi-Automatic Event Annotation}},
year = {2010}
}
@article{Song2016,
abstract = {In this paper, we describe the event nugget an-notation created in support of the pilot Event Nugget Detection evaluation in 2014 and in support of the Event Nugget Detection and Co-reference open evaluation in 2015, which was one of the Knowledge Base Population tracks within the NIST Text Analysis Conference. We present the data volume annotated for both training and evaluation data for the 2015 eval-uation as well as changes to annotation in 2015 as compared to that of 2014. We also analyze the annotation for the 2015 evaluation as an ex-ample to show the annotation challenges and consistency, and identify the event types and subtypes that are most difficult for human an-notators. Finally, we discuss annotation issues that we need to take into consideration in the future.},
author = {Song, Zhiyi and Bies, Ann and Strassel, Stephanie and Ellis, Joe and Mitamura, Teruko and Dang, Hoa and Yamakawa, Yukari and Holm, Sue},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Song et al. - 2016 - Event Nugget and Event Coreference Annotation.pdf:pdf},
pages = {37--45},
title = {{Event Nugget and Event Coreference Annotation}},
year = {2016}
}
@article{Haghighi2009,
abstract = {Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).},
author = {Haghighi, Aria and Klein, Dan},
doi = {10.3115/1699648.1699661},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Haghighi, Klein - 2009 - Simple coreference resolution with rich syntactic and semantic features.pdf:pdf},
isbn = {9781932432633},
journal = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 3 EMNLP 09},
number = {8},
pages = {1152--1161},
title = {{Simple coreference resolution with rich syntactic and semantic features}},
url = {http://portal.acm.org/citation.cfm?doid=1699648.1699661},
volume = {3},
year = {2009}
}
@article{Pouyanfar2016,
abstract = {Numerous deep learning architectures have been designed for a variety of tasks in the past few years. However, it is almost impossible for one model to work well for all kinds of scenarios and datasets. Therefore, we present an ensemble deep learning framework in this paper, which not only decreases the information loss and over-fitting problems caused by single models, but also overcomes the imbalanced data issue in multimedia big data. First, a suite of deep learning algorithms are utilized for deep feature selection. Thereafter, an enhanced ensemble algorithm is developed based on the performance of each single Support Vector Machine classifier on each deep feature set. We evaluate our proposed ensemble deep learning framework on a large and highly imbalanced video dataset containing natural disaster events. Experimental results demonstrate the effectiveness of the proposed framework for semantic event detection, and show how it outperforms several state-of-the-art deep learning architectures, as well as handcrafted features integrated with ensemble and non-ensemble algorithms. {\textcopyright}2016 IEEE.},
author = {Pouyanfar, Samira and Chen, Shu-Ching},
doi = {10.1109/ISM.2016.121},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Pouyanfar, Chen - 2016 - Semantic Event Detection Using Ensemble Deep Learning.pdf:pdf},
isbn = {978-1-5090-4571-6},
journal = {Proceedings of 2016 Ieee International Symposium on Multimedia (Ism)},
keywords = {-Deep learning,Big data,Deep learning,Disasters,Ensemble algorithms,Ensemble learning,Feature extraction,Image retrieval,Imbalanced data,Learning algorithms,Learning architectures,Learning frameworks,Multimedia big data,Natural disasters,Over fitting problem,Semantic event detection,Semantics},
pages = {203--208},
title = {{Semantic Event Detection Using Ensemble Deep Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015165835{\&}doi=10.1109{\%}2FISM.2016.121{\&}partnerID=40{\&}md5=37fd91606327fa40d8981bd94de0651b{\%}0Ahttps://users.cs.fiu.edu/{\%}7B{~}{\%}7Dchens/PDF/ISM06.pdf},
year = {2016}
}
@article{Mihaylov2016,
abstract = {With this work we participate in the TAC-KBP 2016 Event Nugget Track for Event Nugget Detection. We implement a simple but ef-fective system which uses a Bi-Directional LSTM with embeddings short-cuts to the out-put for Event Trigger detection and simple Lo-gistic Regression Classifiers with event trigger context features based on word embeddings for Event Type and Realis detection. Our post-submission models yield state-of-the-art re-sults on the official task dataset. We train and evaluate our system for English and it can eas-ily be adapted for Chinese, Spanish or other languages as our best model uses only word embeddings as external resources.},
author = {Mihaylov, Todor and Frank, Anette},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Mihaylov, Frank - 2016 - AIPHES-HD system at TAC KBP 2016 Neural Event Trigger Detection and Event Type and Realis Disambiguation with W.pdf:pdf},
journal = {Proceedings of the TAC Knowledge Base Population (KBP)},
keywords = {TAC{\_}KBP{\_}2016{\_}events},
mendeley-tags = {TAC{\_}KBP{\_}2016{\_}events},
pages = {1--2},
title = {{AIPHES-HD system at TAC KBP 2016: Neural Event Trigger Detection and Event Type and Realis Disambiguation with Word Embeddings}},
url = {https://tac.nist.gov/publications/2016/participant.papers/TAC2016.aipheshd{\_}t16.proceedings.pdf},
year = {2016}
}
@article{Chen2009,
abstract = {In this paper, we present a Chinese event ex-traction system. We point out a language spe-cific issue in Chinese trigger labeling, and then commit to discussing the contributions of lexical, syntactic and semantic features ap-plied in trigger labeling and argument labeling. As a result, we achieved competitive perform-ance, specifically, F-measure of 59.9 in trigger labeling and F-measure of 43.8 in argument labeling.},
author = {Chen, Zheng and Ji, Heng},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chen, Ji - 2009 - Language Specific Issue and Feature Exploration in Chinese Event Extraction.pdf:pdf},
journal = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers},
number = {June},
pages = {209--212},
title = {{Language Specific Issue and Feature Exploration in Chinese Event Extraction}},
url = {http://aclanthology.info/papers/language-specific-issue-and-feature-exploration-in-chinese-event-extraction},
year = {2009}
}
@article{Lee2012,
abstract = {We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using lin- ear regression to model cluster merge opera- tions. As clusters are built, information flows between entity and event clusters through fea- tures that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable doc- uments, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.},
author = {Lee, Heeyoung and Recasens, Marta and Chang, Angel and Surdeanu, Mihai and Jurafsky, Dan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lee et al. - 2012 - Joint Entity and Event Coreference Resolution across Documents.pdf:pdf},
isbn = {9781937284435},
journal = {(EMNLP-CoNLL 2012) Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {July},
pages = {489--500},
title = {{Joint Entity and Event Coreference Resolution across Documents}},
url = {http://www.aclweb.org/anthology/D12-1045},
year = {2012}
}
@misc{VanHee2014,
author = {{Van Hee}, Cynthia and {Van de Kauter}, Marjan and {De Clercq}, Orph{\'{e}}e and Lefever, Els and Hoste, Veronique},
isbn = {9781941643242},
language = {eng},
publisher = {Association for Computational Linguistics 2014},
title = {{LT3: sentiment classification in user-generated content using a rich feature set}},
url = {http://lib.ugent.be/catalog/pug01:5684543},
year = {2014}
}
@article{Liu2016,
author = {Liu, Zhengzhong and Araki, Jun and Mitamura, Teruko and Hovy, Eduard},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Liu et al. - 2016 - CMU-LTI at KBP 2016 Event Nugget Track.pdf:pdf},
title = {{CMU-LTI at KBP 2016 Event Nugget Track}},
year = {2016}
}
@article{VandeKauter2015,
abstract = {{\textcopyright} 2015, Springer Science+Business Media Dordrecht.We present a fine-grained scheme for the annotation of polar sentiment in text, that accounts for explicit sentiment (so-called private states), as well as implicit expressions of sentiment (polar facts). Polar expressions are annotated below sentence level and classified according to their subjectivity status. Additionally, they are linked to one or more targets with a specific polar orientation and intensity. Other components of the annotation scheme include source attribution and the identification and classification of expressions that modify polarity. In previous research, little attention has been given to implicit sentiment, which represents a substantial amount of the polar expressions encountered in our data. An English and Dutch corpus of financial newswire text, consisting of over 45,000 words each, was annotated using our scheme. A subset of this corpus was used to conduct an inter-annotator agreement study, which demonstrated that the proposed scheme can be used to reliably annotate explicit and implicit sentiment in real-world textual data, making the created corpora a useful resource for sentiment analysis.},
author = {{Van de Kauter}, Marjan and Desmet, Bart and Hoste, Veronique},
doi = {10.1007/s10579-015-9297-4},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Van de Kauter, Desmet, Hoste - 2015 - The good, the bad and the implicit a comprehensive approach to annotating explicit and implicit se.pdf:pdf},
issn = {15728412},
journal = {Language Resources and Evaluation},
keywords = {Corpus annotation,Natural language processing,Polarity,Sentiment analysis},
number = {49},
pages = {685--720},
title = {{The good, the bad and the implicit: a comprehensive approach to annotating explicit and implicit sentiment}},
year = {2015}
}
@article{Peng2016,
abstract = {An important aspect of natural language un-derstanding involves recognizing and catego-rizing events and the relations among them. However, these tasks are quite subtle and an-notating training data for machine learning based approaches is an expensive task, re-sulting in supervised systems that attempt to learn complex models from small amounts of data, which they over-fit. This paper addresses this challenge by developing an event detec-tion and co-reference system with minimal su-pervision, in the form of a few event exam-ples. We view these tasks as semantic similar-ity problems between event mentions or event mentions and an ontology of types, thus fa-cilitating the use of large amounts of out of domain text data. Notably, our semantic re-latedness function exploits the structure of the text by making use of a semantic-role-labeling based representation of an event. We show that our approach to event detection is competitive with the top supervised meth-ods. More significantly, we outperform state-of-the-art supervised methods for event co-reference on benchmark data sets, and support significantly better transfer across domains.},
author = {Peng, Haoruo and Song, Yangqi and Roth, Dan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Peng, Song, Roth - 2016 - Event Detection and Co-reference with Minimal Supervision.pdf:pdf},
journal = {Emnlp},
pages = {392--402},
title = {{Event Detection and Co-reference with Minimal Supervision}},
url = {http://cogcomp.cs.illinois.edu/papers/PengSoRo16.pdf},
year = {2016}
}
@article{Pradhan2007,
abstract = {Most research in the field of anaphora or coreference detection has been limited to noun phrase coreference, usually on a restricted set of entities, such as ACE entities. In part, this has been due to the lack of corpus resources tagged with general anaphoric coreference. The OntoNotes project is creating a large-scale, accurate corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a limited set of entity types. The coreference layer in OntoNotes constitutes one part of a multi-layer, integrated annotation of shallow semantic structure in text. This paper presents an initial model for unrestricted coreference based on this data that uses a machine learning architecture with state-of-the-art features. Significant improvements can be expected from using such cross-layer information for training predictive models. This paper describes the coreference annotation in OntoNotes, presents the baseline model, and provides an analysis of the contribution of this new resource in the context of recent MUC and ACE results.},
author = {Pradhan, Sameer S. and Ramshaw, Lance and Weischedel, Ralph and MacBride, Jessica and Micciulla, Linnea},
doi = {10.1109/ICSC.2007.93},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Pradhan et al. - 2007 - Unrestricted coreference Identifying entities and events in ontonotes.pdf:pdf},
isbn = {0769529976},
journal = {ICSC 2007 International Conference on Semantic Computing},
pages = {446--453},
title = {{Unrestricted coreference: Identifying entities and events in ontonotes}},
year = {2007}
}
@article{Gupta2009,
author = {Gupta, Prashant},
doi = {10.3115/1667583.1667697},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Gupta - 2009 - Predicting Unknown Time Arguments based on Cross-Event Propagation.pdf:pdf},
isbn = {9781617382581},
journal = {ACL Short Paper},
number = {August},
pages = {369--372},
title = {{Predicting Unknown Time Arguments based on Cross-Event Propagation}},
year = {2009}
}
@article{Yangarber2000a,
abstract = {Information Extraction (IE) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base---a time-consuming and expensive process. We have implemented a system for finding patterns automatically from un-annotated text. Starting with a small initial set of seed patterns proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall.},
author = {Yangarber, Roman and Grishman, Ralph},
doi = {10.3115/974147.974186},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yangarber, Grishman - 2000 - Unsupervised discovery of scenario-level patterns for information extraction.pdf:pdf},
journal = {18th International Conference on Computational Linguistics, COLING 2000},
pages = {940--946},
title = {{Unsupervised discovery of scenario-level patterns for information extraction}},
url = {http://dl.acm.org/citation.cfm?id=974186},
year = {2000}
}
@inproceedings{lu2017learning,
author = {Lu, Jing and Ng, Vincent},
booktitle = {Machine Learning and Applications (ICMLA), 2017 16th IEEE International Conference on},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lu, Ng - 2017 - Learning Antecedent Structures for Event Coreference Resolution.pdf:pdf},
organization = {IEEE},
pages = {113--118},
title = {{Learning Antecedent Structures for Event Coreference Resolution}},
year = {2017}
}
@article{DeClercq2015a,
abstract = {The LT3 system perceives ABSA as a task consisting of three main subtasks, which have to be tackled incrementally, namely aspect term extraction, aggregation and polarity clas-sification. For the first two steps, we see that employing a hybrid terminology extrac-tion system leads to promising results, espe-cially when it comes to recall. For the polar-ity classification, we show that it is possible to gain satisfying accuracies, even on out-of-domain data, with a basic model employing only lexical information.},
author = {{De Clercq}, Orph{\'{e}}e and {Van De Kauter}, Marjan and Lefever, Els and Hoste, Veronique},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/De Clercq et al. - 2015 - Applying hybrid terminology extraction to aspect-based sentiment analysis.pdf:pdf},
title = {{LT3: Applying Hybrid Terminology Extraction to Aspect Based Sentiment Analysis}},
year = {2015}
}
@article{Sammons2015,
abstract = {This paper describes University of Illi-nois's Cognitive Computation Group (UI-CCG)'s submissions for three TAC tracks: Event Nugget Detection/Co-reference; Entity Discovery and Linking (EDL); and Slot Filler Validation (SFV). The Event Nugget Detection and Co-reference system employs a supervised model for event nugget detection with rich lexical and semantic features while we ex-periment with both supervised and unsu-pervised event co-reference methods. We also utilize ACE2005 data as an additional source and use several domain adaptation techniques to improve our system's perfor-mance. The Entity Discovery and Linking sys-tem focuses on solving the Spanish sub-task. The system uses Google Transla-tion to translate Spanish documents into English and then apply Illinois Wikifier to identify entity mentions and disambiguate them to Wikipedia entries. It outperforms other participants on both linking and clus-teringevaluations.},
author = {Sammons, Mark and Peng, Haoruo and Song, Yangqiu and Upadhyay, Shyam and Tsai, Chen-tse and Reddy, Pavankumar and Roy, Subhro and Roth, Dan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Sammons et al. - 2015 - Illinois CCG TAC 2015 Event Nugget , Entity Discovery and Linking , and Slot Filler Validation Systems.pdf:pdf},
journal = {Proceedings of Text Analysis Conference},
title = {{Illinois CCG TAC 2015 Event Nugget , Entity Discovery and Linking , and Slot Filler Validation Systems}},
year = {2015}
}
@article{Chen2009a,
abstract = {In contrast to entity coreference resolution, event coreference resolution has not received great attention from researchers. In this paper, we first demonstrate the diverse scenarios of event coreference by an example. We then model event coreference resolution as a spectral graph clustering problem and evaluate the clustering algorithm on ground truth event mentions using ECM F-Measure. We obtain the ECM-F scores of 0.8363 and 0.8312 respectively by using two methods for computing coreference matrices.},
author = {Chen, Zheng and Ji, H},
doi = {10.3115/1708124.1708135},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chen, Ji - 2009 - Graph-based event coreference resolution.pdf:pdf},
isbn = {9781932432541},
journal = {Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing},
number = {August},
pages = {54--57},
title = {{Graph-based event coreference resolution}},
year = {2009}
}
@article{Hong2011,
abstract = {Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6{\%} gain in trigger (event) identification, and more than 11.8{\%} gain for argument (role) classification in ACE event extraction.},
author = {Hong, Yu and Zhang, Jianfeng and Ma, Bin and Yao, Jianmin and Zhou, Guodong and Zhu, Qiaoming},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Hong et al. - 2011 - Using Cross-Entity Inference to Improve Event Extraction.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
number = {4},
pages = {1127--1136},
title = {{Using Cross-Entity Inference to Improve Event Extraction}},
url = {http://dl.acm.org/citation.cfm?id=2002615},
year = {2011}
}
@article{Ganea2015,
abstract = {Many fundamental problems in natural language processing rely on determining what entities appear in a given text. Commonly referenced as entity linking, this step is a fundamental component of many NLP tasks such as text understanding, automatic summarization, semantic search or machine translation. Name ambiguity, word polysemy, context dependencies and a heavy-tailed distribution of entities contribute to the complexity of this problem. We here propose a probabilistic approach that makes use of an effective graphical model to perform collective entity disambiguation. Input mentions (i.e.,{\~{}}linkable token spans) are disambiguated jointly across an entire document by combining a document-level prior of entity co-occurrences with local information captured from mentions and their surrounding context. The model is based on simple sufficient statistics extracted from data, thus relying on few parameters to be learned. Our method does not require extensive feature engineering, nor an expensive training procedure. We use loopy belief propagation to perform approximate inference. The low complexity of our model makes this step sufficiently fast for real-time usage. We demonstrate the accuracy of our approach on a wide range of benchmark datasets, showing that it matches, and in many cases outperforms, existing state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1509.02301},
author = {Ganea, Octavian-Eugen and Ganea, Marina and Lucchi, Aurelien and Eickhoff, Carsten and Hofmann, Thomas},
doi = {10.1145/2872427.2882988},
eprint = {1509.02301},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ganea et al. - 2015 - Probabilistic Bag-Of-Hyperlinks Model for Entity Linking.pdf:pdf},
isbn = {9781450341431},
keywords = {abilistic graphical models,approximate inference,entity disambiguation,entity linking,loopy,prob-,wikification},
title = {{Probabilistic Bag-Of-Hyperlinks Model for Entity Linking}},
url = {http://arxiv.org/abs/1509.02301},
year = {2015}
}
@article{Cao2015,
author = {Cao, Kai and Li, Xiang and Fan, Miao and Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Cao et al. - 2015 - Improving Event Detection with Active Learning.pdf:pdf},
pages = {72--77},
title = {{Improving Event Detection with Active Learning}},
year = {2015}
}
@article{Aguilar2014,
author = {Aguilar, Jacqueline and Beller, Charley and McNamee, Paul and {Van Durme}, Benjamin and Strassel, Stephanie and Song, Zhiyi and Ellis, Joe},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Aguilar et al. - 2014 - A Comparison of the Events and Relations Across ACE , ERE , TAC-KBP , and FrameNet Annotation Standards.pdf:pdf;:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Aguilar et al. - 2014 - A Comparison of the Events and Relations Across ACE , ERE , TAC-KBP , and FrameNet Annotation Standards(2).pdf:pdf},
title = {{A Comparison of the Events and Relations Across ACE , ERE , TAC-KBP , and FrameNet Annotation Standards}},
year = {2014}
}
@article{Che2010,
abstract = {LTP (Language Technology Platform) is an integrated Chinese processing platform which includes a suite of high perfor-mance natural language processing (NLP) modules and relevant corpora. Espe-cially for the syntactic and semantic pars-ing modules, we achieved good results in some relevant evaluations, such as CoNLL and SemEval. Based on XML in-ternal data representation, users can easily use these modules and corpora by invok-ing DLL (Dynamic Link Library) or Web service APIs (Application Program Inter-face), and view the processing results di-rectly by the visualization tool.},
author = {Che, Wanxiang and Li, Zhenghua and Liu, Ting},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Che, Li, Liu - 2010 - LTP A Chinese Language Technology Platform.pdf:pdf},
journal = {Coling},
number = {August},
pages = {13--16},
title = {{LTP: A Chinese Language Technology Platform}},
url = {http://anthology.aclweb.org/C/C10/C10-3004.pdf},
year = {2010}
}
@article{Lu2012,
abstract = {This paper presents a novel sequence label-ing model based on the latent-variable semi-Markov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are avail-able. Our key contribution is a novel learning framework called structured preference mod-eling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the suc-cess of our task. Our model, trained with-out annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised ap-proaches.},
author = {Lu, Wei and Roth, Dan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lu, Roth - 2012 - Automatic Event Extraction with Structured Preference Modeling.pdf:pdf},
isbn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
number = {July},
pages = {835--844},
title = {{Automatic Event Extraction with Structured Preference Modeling}},
url = {http://aclweb.org/anthology/P12-1088{\%}5Cnhttp://aclanthology.info/papers/automatic-event-extraction-with-structured-preference-modeling},
year = {2012}
}
@article{Kompan2010,
abstract = {The information overloading is one of the serious problems nowadays. We can see it in various domains including business. Especially news represent area where information overload currently prevents effective information gathering on daily basis. This is more significant in connection to the web and news web-based portals, where the quality of the news portal is commonly measured mainly by the amount of news added to the site. Then the most renowned news portals add hundreds of new articles daily. The classical solution usually used to solve the information overload is a recommendation, especially personalized recommendation. In this paper we present an approach for fast content-based news recommendation based on cosine-similarity search and effective representation of the news. We experimented with proposed method in an environment of largest electronic Slovakia newspaper and present results of the experiments.},
author = {Kompan, Michal and Bielikov{\'{a}}, M{\'{a}}ria},
doi = {10.1007/978-3-642-15208-5_6},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Kompan, Bielikov{\'{a}} - 2010 - Content-based news recommendation.pdf:pdf},
isbn = {3642152074},
issn = {18651348},
journal = {Lecture Notes in Business Information Processing},
keywords = {article similarity,news,personalization,recommendation,user model,vector representation},
pages = {61--72},
title = {{Content-based news recommendation}},
volume = {61 LNBIP},
year = {2010}
}
@article{Grishman2010,
author = {Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Grishman - 2010 - The Impact of Task and Corpus on Event Extraction Systems.pdf:pdf},
isbn = {2-9517408-6-7},
journal = {Lrec},
pages = {2928--2931},
title = {{The Impact of Task and Corpus on Event Extraction Systems}},
year = {2010}
}
@article{Dubbin2015,
author = {Dubbin, Greg and Bhatia, Archna and Dorr, Bonnie and Dalton, Adam and Hollingshead, Kristy and Kandaswamy, Suriya and Perera, Ian and Hwang, Jena D},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Dubbin et al. - 2015 - Improving DISCERN with Deep Learning.pdf:pdf},
keywords = {TAC{\_}KBP{\_}2016{\_}events},
mendeley-tags = {TAC{\_}KBP{\_}2016{\_}events},
title = {{Improving DISCERN with Deep Learning}},
year = {2015}
}
@phdthesis{Schluter2017,
author = {Schl{\"{u}}ter, Jan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Schl{\"{u}}ter - 2017 - Deep Learning for Event Detection, Sequence Labelling and Similarity Estimation in Music Signals.pdf:pdf},
title = {{Deep Learning for Event Detection, Sequence Labelling and Similarity Estimation in Music Signals}},
url = {http://ofai.at/{~}jan.schlueter/pubs/phd/phd.pdf},
year = {2017}
}
@article{Ji2010b,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at TAC 2010. The main goal of KBP is to promote research in discovering facts about entities and expanding a structured knowledge base with this information. A large source collection of newswire and web documents is provided for systems to discover information. Attributes (a.k.a. “slots”) derived from Wikipedia infoboxes are used to create the reference knowledge base (KB). KBP2010 includes the following four tasks: (1) Regular Entity Linking, where names must be aligned to entities in the KB; (2) Optional Entity linking, without using Wikipedia texts; (3) Regular Slot Filling, which requires a system to automatically discover the attributes of specified entities from the source document collection and use them to expand the KB; (4) Surprise Slot Filling, which requires a system to return answers regarding new slot types within a short time period. KBP2010 has attracted many participants (over 45 teams registered for KBP 2010 (not including the RTEKBP Validation Pilot task), among which 23 teams submitted results). In this paper we provide an overview of the task definition and annotation challenges associated with KBP2010. Then we summarize the evaluation results and discuss the lessons that we have learned based on detailed analysis.},
author = {Ji, Heng and Grishman, Ralph and Dang, Hoa Trang and Griffitt, Kira and Ellis, Joe},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji et al. - 2010 - Overview of the TAC 2010 Knowledge Base Population Track(2).pdf:pdf},
isbn = {1117458812938},
journal = {Tac 2010},
number = {November},
title = {{Overview of the TAC 2010 Knowledge Base Population Track}},
year = {2010}
}
@article{Allen2008,
abstract = {We describe a graphical logical formas a semantic representation for text understanding. This representation was designed to bridge the gap be- tween highly expressive "deep" representations of logical forms andmore shallow semantic encodings such as word senses and semantic relations. It preserves rich semantic content while allowing for compact ambigu- ity encoding and viable partial representations. We describe our system for semantic text processing, which has the TRIPS parser at the core, augmented with statistical preprocessing techniques and online lexical lookup. We also present an evaluation metric for the representation and use it to evaluate the performance of the TRIPS parser on the common task paragraphs. 343},
author = {Allen, James F. and Swift, Mary and de Beaumont, Will},
doi = {10.3115/1626481.1626508},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Allen, Swift, de Beaumont - 2008 - Deep semantic analysis of text.pdf:pdf},
journal = {Proceedings of the 2008 Conference on Semantics in Text Processing - STEP '08},
pages = {343--354},
title = {{Deep semantic analysis of text}},
url = {http://portal.acm.org/citation.cfm?doid=1626481.1626508},
year = {2008}
}
@article{Vossen2016,
abstract = {In this article, we describe a system that reads news articles in four different languages and detects what happened, who is involved, where and when. This event-centric information is represented as episodic situational knowledge on individuals in an interoperable RDF format that allows for reasoning on the implications of the events. Our system covers the complete path from unstructured text to structured knowledge, for which we defined a formal model that links interpreted textual mentions of things to their representation as instances. The model forms the skeleton for interoperable interpretation across different sources and languages. The real content, however, is defined using multilingual and cross-lingual knowledge resources, both semantic and episodic. We explain how these knowledge resources are used for the processing of text and ultimately define the actual content of the episodic situational knowledge that is reported in the news. The knowledge and model in our system can be seen as an example how the Semantic Web helps NLP. However, our systems also generate massive episodic knowledge of the same type as the Semantic Web is built on. We thus envision a cycle of knowledge acquisition and NLP improvement on a massive scale. This article reports on the details of the system but also on the performance of various high-level components. We demonstrate that our system performs at state-of-the-art level for various subtasks in the four languages of the project, but that we also consider the full integration of these tasks in an overall system with the purpose of reading text. We applied our system to millions of news articles, generating billions of triples expressing formal semantic properties. This shows the capacity of the system to perform at an unprecedented scale.},
author = {Vossen, Piek and Agerri, Rodrigo and Aldabe, Itziar and Cybulska, Agata and van Erp, Marieke and Fokkens, Antske and Laparra, Egoitz and Minard, Anne Lyse and {Palmero Aprosio}, Alessio and Rigau, German and Rospocher, Marco and Segers, Roxane},
doi = {10.1016/j.knosys.2016.07.013},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Vossen et al. - 2016 - NewsReader Using knowledge resources in a cross-lingual reading machine to generate more knowledge from massive s.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Cross-lingual interopearbility,Event extraction,Knowledge resources,Natural language processing,Semantic web},
title = {{NewsReader: Using knowledge resources in a cross-lingual reading machine to generate more knowledge from massive streams of news}},
year = {2016}
}
@article{Ferguson2016,
author = {Ferguson, James and Lockard, Colin and Hawkins, Natalie and Soderland, Stephen and Hajishirzi, Hannaneh and Weld, Daniel S.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ferguson et al. - 2016 - University of Washington TAC-KBP 2016 System Description.pdf:pdf},
number = {4},
title = {{University of Washington TAC-KBP 2016 System Description}},
year = {2016}
}
@article{Liao2010,
abstract = {Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0{\%} (absolute) gain in trigger (event) classification, and more than 8{\%} gain for argument (role) classification in ACE event extraction.},
author = {Liao, Shasha and York, New and Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Liao, York, Grishman - 2010 - Using Document Level Cross-Event Inference to Improve Event Extraction.pdf:pdf},
isbn = {9781617388088},
journal = {Computational Linguistics},
number = {July},
pages = {789--797},
title = {{Using Document Level Cross-Event Inference to Improve Event Extraction}},
year = {2010}
}
@book{Jing2013,
abstract = {Information extraction is the task of finding structured information from unstructured or semi-structured text. It is an important task in text mining and has been extensively studied in various research commu- nities including natural language processing, information retrieval and Web mining. It has a wide range of applications in domains such as biomedical literature mining and business intelligence. Two fundamen- tal tasks of information extraction are named entity recognition and relation extraction. The former refers to finding names of entities such as people, organizations and locations. The latter refers to finding the semantic relations such as FounderOf and HeadquarteredIn between en- tities. In this chapter we provide a survey of the major work on named entity recognition and relation extraction in the past few decades, with a focus on work from the natural language processing community.},
address = {Boston, MA},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jing, Jiang},
booktitle = {Mining Text Data},
chapter = {2},
doi = {10.1007/978-1-4614-3223-4},
editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Jing - 2012 - Mining Text Data.pdf:pdf},
isbn = {978-1-4614-3222-7},
issn = {1098-6596},
keywords = {information extraction,named entity recognition,relation extraction},
pages = {11--41},
pmid = {25246403},
publisher = {Springer US},
title = {{Mining Text Data}},
url = {http://link.springer.com/10.1007/978-1-4614-3223-4},
year = {2012}
}
@article{Stoyanov2009,
abstract = {We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.},
author = {Stoyanov, Veselin and Gilbert, Nathan and Cardie, Claire and Riloff, Ellen},
doi = {10.3115/1690219.1690238},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Stoyanov et al. - 2009 - Conundrums in Noun Phrase Coreference Resolution Making Sense of the State-of-the-Art.pdf:pdf},
isbn = {9781932432466},
journal = {Differences},
number = {August},
pages = {656--664},
title = {{Conundrums in Noun Phrase Coreference Resolution : Making Sense of the State-of-the-Art}},
url = {http://www.aclweb.org/anthology/P/P09/P09-1074},
year = {2009}
}
@article{Chen2017,
abstract = {Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training da-ta is expensive to produce, in low cov-erage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data label-ing problem, we propose to automatically label training data for event extraction vi-a world knowledge and linguistic knowl-edge, which can detect key arguments and trigger words for each event type and em-ploy them to label events in texts auto-matically. The experimental results show that the quality of our large scale automat-ically labeled data is competitive with e-laborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.},
author = {Chen, Yubo and Liu, Shulin and Zhang, Xiang and Liu, Kang and Zhao, Jun},
doi = {10.18653/v1/P17-1038},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chen et al. - 2017 - Automatically Labeled Data Generation for Large Scale Event Extraction.pdf:pdf},
journal = {Acl},
pages = {409--419},
title = {{Automatically Labeled Data Generation for Large Scale Event Extraction}},
url = {https://doi.org/10.18653/v1/P17-1038},
year = {2017}
}
@article{Hogenboom2011,
abstract = {One common application of text mining is event extraction, which encompasses deducing specific knowledge concerning incidents re- ferred to in texts. Event extraction can be applied to various types of written text, e.g., (online) news messages, blogs, and manuscripts. This literature survey reviews text mining techniques that are employed for various event extraction purposes. It provides general guidelines on how to choose a particular event extraction technique depending on the user, the available content, and the scenario of use.},
author = {Hogenboom, Frederik and Frasincar, Flavius and Kaymak, Uzay and {De Jong}, Franciska},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Hogenboom et al. - 2011 - An overview of event extraction from text.pdf:pdf},
isbn = {1467392006},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {48--57},
title = {{An overview of event extraction from text}},
volume = {779},
year = {2011}
}
@article{Satyapanich2016,
author = {Satyapanich, Taneeya and Finin, Tim},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Satyapanich, Finin - 2016 - Event Nugget Detection Task UMBC systems.pdf:pdf},
title = {{Event Nugget Detection Task : UMBC systems}},
year = {2016}
}
@article{Bejan2010,
abstract = {This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.},
author = {Bejan, Cosmin and Harabagiu, Sanda},
doi = {10.1162/COLI_a_00174},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Bejan, Harabagiu - 2010 - Unsupervised Event Coreference Resolution with Rich Linguistic Features.pdf:pdf},
isbn = {9781617388088},
issn = {0891-2017},
journal = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
keywords = {upervised event coreference resolution,with rich linguistic features},
number = {July},
pages = {1412--1422},
title = {{Unsupervised Event Coreference Resolution with Rich Linguistic Features}},
url = {http://www.aclweb.org/anthology/P10-1143},
year = {2010}
}
@misc{Consortium2008a,
abstract = {The Entity Detection task requires that selected types of entities mentioned in the source data be detected, their sense disambiguated, and that selected attributes of these entities be extracted and merged into a unified representation for each entity.},
booktitle = {Facilities},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Unknown - 2008 - ACE English Annotation Guidelines for Events (v5.4.3).pdf:pdf},
pages = {72},
publisher = {Linguistics Data Consortium},
title = {{ACE English Annotation Guidelines for Events (v5.4.3)}},
url = {http://projects.ldc.upenn.edu/ace/docs/English-Entities-Guidelines{\_}v6.6.pdf},
year = {2008}
}
@article{Lin,
abstract = {Abstract—Supervised learning methods rely heavily on the quantity and quality of annotations provided by humans. As more natural language processing systems utilize human labeled data, it becomes beneficial to discover some hidden privileged knowledge from human annotators. In a traditional framework, a human annotator and a system are treated as isolated black-boxes. We propose better utilization of the valuable knowledge possessed by human annotators in the system development. This can be achieved by asking annotators to provide “rich annotations” for feature encoding. The rich annotations can come at multiple levels such as highlighting and generalizing contexts, and providing highlevel comments. We propose a general framework to exploit such rich annotations from human annotators. This framework is a novel extension of our previous work by adding {\ldots}},
author = {Li, Xiang and Ji, Heng and Farooq, Faisal and Li, Hao and Lin, Wen-Pin and Yu, Shipeng},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li et al. - 2012 - Rich Annotation Guided Learning.pdf:pdf},
journal = {International Journal On Advances in Intelligent Systems},
keywords = {-rich annotation,feature engineering},
title = {{Rich Annotation Guided Learning}},
year = {2012}
}
@article{Mitamura,
abstract = {In this paper, we describe the second Event Nugget evaluation track for Knowledge Base Population(KBP) at TAC 2016. This year we extend the Event Nugget task to a trilingual setting: English, Chinese and Spanish. All the Event Nugget sub-tasks now require end-to-end processing from raw text. This task has attracted a lot of participation and intrigued in-teresting research problems. In this paper we try to provide an overview on the task defini-tion, data annotation, evaluation and trending research methods. We further discuss issues re-lated to the annotation process and the current restricted evaluation scope. With the lessons learned, we hope the next KBP Event Nugget task can incorporate more complex event rela-tions on a larger scale.},
author = {Mitamura, Teruko and Liu, Zhengzhong and Hovy, Eduard},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Mitamura, Liu, Hovy - 2016 - Overview of TAC-KBP 2016 Event Nugget Track.pdf:pdf},
journal = {Tac Kbp 2016},
title = {{Overview of TAC-KBP 2016 Event Nugget Track}},
year = {2016}
}
@article{Chan2011,
abstract = {In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.},
author = {Chan, Yee Seng and Roth, Dan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chan, Roth - 2011 - Exploiting Syntactico-semantic Structures for Relation Extraction.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
keywords = {KM,TAL},
pages = {551--560},
title = {{Exploiting Syntactico-semantic Structures for Relation Extraction}},
url = {http://dl.acm.org/citation.cfm?id=2002472.2002542},
year = {2011}
}
@article{Maslennikov2007,
abstract = {Extraction of relations between entities is an important part of Information Extraction on free text. Previous methods are mostly based on statistical correlation and depend-ency relations between entities. This paper re-examines the problem at the multi-resolution layers of phrase, clause and sen-tence using dependency and discourse rela-tions. Our multi-resolution framework ARE (Anchor and Relation) uses clausal relations in 2 ways: 1) to filter noisy de-pendency paths; and 2) to increase reliabil-ity of dependency path extraction. The re-sulting system outperforms the previous approaches by 3{\%}, 7{\%}, 4{\%} on MUC4, MUC6 and ACE RDC domains respec-tively.},
author = {Maslennikov, Mstislav and Chua, Tat-Seng},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Maslennikov, Chua - 2007 - A Multi-resolution Framework for Information Extraction from Free Text.pdf:pdf},
number = {June},
pages = {592--599},
title = {{A Multi-resolution Framework for Information Extraction from Free Text}},
year = {2007}
}
@article{Mitamura2015,
author = {Mitamura, Teruko and Liu, Zhengzhong and Hovy, Eduard},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Mitamura, Liu, Hovy - 2015 - Overview of TAC KBP 2015 Event Nugget Track.pdf:pdf},
journal = {Kbp Tac 2015},
pages = {1--31},
title = {{Overview of TAC KBP 2015 Event Nugget Track}},
year = {2015}
}
@article{Sha2016,
abstract = {Event extraction is a particularly challenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classification), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization- Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents.},
author = {Sha, Lei and Liu, Jing and Lin, Chin-Yew and Li, Sujian and Chang, Baobao and Sui, Zhifang},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Sha et al. - 2016 - RBPB Regularization-Based Pattern Balancing Method for Event Extraction.pdf:pdf},
isbn = {9781510827585},
journal = {Acl},
pages = {1224--1234},
title = {{RBPB: Regularization-Based Pattern Balancing Method for Event Extraction}},
url = {http://www.aclweb.org/anthology/P16-1116},
year = {2016}
}
@article{Das:2014:FP:2645242.2645244,
address = {Cambridge, MA, USA},
author = {Das, Dipanjan and Chen, Desai and Martins, Andr{\'{e}} F T and Schneider, Nathan and Smith, Noah A},
doi = {10.1162/COLI_a_00163},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = {mar},
number = {1},
pages = {9--56},
publisher = {MIT Press},
title = {{Frame-semantic Parsing}},
url = {http://dx.doi.org/10.1162/COLI{\_}a{\_}00163},
volume = {40},
year = {2014}
}
@article{Mohammad2016,
abstract = {Here for the first time we present a shared task on detecting stance from tweets: given a tweet and a target entity (person, organiza-tion, etc.), automatic natural language systems must determine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely. The target of interest may or may not be referred to in the tweet, and it may or may not be the tar-get of opinion. Two tasks are proposed. Task A is a traditional supervised classification task where 70{\%} of the annotated data for a target is used as training and the rest for testing. For Task B, we use as test data all of the instances for a new target (not used in task A) and no training data is provided. Our shared task re-ceived submissions from 19 teams for Task A and from 9 teams for Task B. The highest clas-sification F-score obtained was 67.82 for Task A and 56.28 for Task B. However, systems found it markedly more difficult to infer stance towards the target of interest from tweets that express opinion towards another entity.},
author = {Mohammad, Saif M and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
doi = {10.18653/v1/S16-1003},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Mohammad et al. - 2016 - SemEval-2016 Task 6 Detecting Stance in Tweets.pdf:pdf},
isbn = {9781941643952},
journal = {Proceedings of SemEval},
number = {July},
pages = {31--41},
title = {{SemEval-2016 Task 6: Detecting Stance in Tweets}},
url = {http://www.saifmohammad.com/WebDocs/semeval2016-task6-stance.pdf{\%}0Ahttp://m-mitchell.com/NAACL-2016/SemEval/pdf/SemEval03.pdf},
volume = {16},
year = {2016}
}
@article{Bies2016,
abstract = {This paper will discuss and compare event representations across a variety of types of event annotation: Rich Entities, Relations, and Events (Rich ERE), Light Entities, Relations, and Events (Light ERE), Event Nugget (EN), Event Argument Extraction (EAE), Richer Event Descriptions (RED), and Event-Event Relations (EER). Comparisons of event rep-resentations are presented, along with a com-parison of data annotated according to each event representation. An event annotation ex-periment is also discussed, including annota-tion for all of these representations on the same set of sample data, with the purpose of being able to compare actual annotation across all of these approaches as directly as possible. We walk through a brief example to illustrate the various annotation approaches, and to show the intersections among the vari-ous annotated data sets.},
author = {Bies, Ann and Song, Zhiyi and Getman, Jeremy and Ellis, Joe and Mott, Justin and Strassel, Stephanie and Palmer, Martha and Mitamura, Teruko and Freedman, Marjorie and Ji, Heng and {O 'gorman}, Tim},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Bies et al. - 2016 - A Comparison of Event Representations in DEFT.pdf:pdf},
pages = {27--36},
title = {{A Comparison of Event Representations in DEFT}},
year = {2016}
}
@article{Fan2015,
abstract = {This paper contributes a joint embedding model for predicting relations between a pair of entities in the scenario of relation inference. It differs from most stand-alone approaches which separately operate on either knowledge bases or free texts. The proposed model simultaneously learns low-dimensional vector representations for both triplets in knowledge repositories and the mentions of relations in free texts, so that we can leverage the evidence both resources to make more accurate predictions. We use NELL to evaluate the performance of our approach, compared with cutting-edge methods. Results of extensive experiments show that our model achieves significant improvement on relation extraction.},
archivePrefix = {arXiv},
arxivId = {1504.01683},
author = {Fan, Miao and Cao, Kai and He, Yifan and Grishman, Ralph},
eprint = {1504.01683},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Fan et al. - 2015 - Jointly Embedding Relations and Mentions for Knowledge Population.pdf:pdf},
issn = {13138502},
title = {{Jointly Embedding Relations and Mentions for Knowledge Population}},
url = {http://arxiv.org/abs/1504.01683},
year = {2015}
}
@article{IV2014,
abstract = {This article discusses the requirements of a formal specification for the annotation of temporal information in clinical narratives. We discuss the implementation and extension of ISO-TimeML for annotating a corpus of clinical notes, known as the THYME corpus. To reflect the information task and the heavily inference-based reasoning demands in the domain, a new annotation guideline has been developed, “the THYME Guidelines to ISO-TimeML (THYME-TimeML)”. To clarify what relations merit annotation, we distinguish between linguistically-derived and inferentially-derived temporal orderings in the text. We also apply a top performing TempEval 2013 system against this new resource to measure the difficulty of adapting systems to the clinical domain. The corpus is available to the community and has been proposed for use in a SemEval 2015 task. },
author = {IV, William F. Styler and Bethard, Steven and Finan, Sean and Palmer, Martha and Pradhan, Sameer and de Groen, Piet C. and Erickson, Brad and Miller, Timothy and Lin, Chen and Savova, Guergana and Pustejovsky, James},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/IV et al. - 2014 - Temporal Annotation in the Clinical Domain.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
number = {1},
pages = {143--154},
pmid = {29082229},
title = {{Temporal Annotation in the Clinical Domain}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/305},
volume = {2},
year = {2014}
}
@inproceedings{Segers2016,
abstract = {This paper presents the Event and Implied Situation Ontology (ESO), a resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology reuses and maps across existing resources such as WordNet, SUMO, VerbNet, Prop-Bank and FrameNet. We describe how ESO is injected into a new version of the Predicate Matrix and illustrate how these resources are used to detect information in large document collections that otherwise would have remained implicit. The model targets interpretations of situations rather than the semantics of verbs per se. The event is interpreted as a situation using RDF taking all event components into account. Hence, the ontology and the linked resources need to be considered from the perspective of this interpretation model.},
author = {Segers, R. and Laparra, Egoitz and Rospocher, M. and Vossen, Piek and Rigau, G. and Ilievski, Filip},
booktitle = {Proceedings of the 8th Global WordNet Conference, GWC 2016},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Segers et al. - 2016 - The predicate matrix and the event and implied situation ontology Making more of events.pdf:pdf},
isbn = {9789730207286},
title = {{The predicate matrix and the event and implied situation ontology: Making more of events}},
year = {2016}
}
@inproceedings{VanErp,
abstract = {Entity linking has become a popular task in both natural language processing and semantic web communities. However, we find that the benchmark datasets for entity linking tasks do not accurately evaluate entity linking systems. In this paper, we aim to chart the strengths and weaknesses of current benchmark datasets and sketch a roadmap for the community to devise better benchmark datasets.},
author = {van Erp, Marieke and Mendes, Pablo N and Paulheim, Heiko and Ilievski, Filip and Plu, Julien and Rizzo, Giuseppe},
booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/van Erp et al. - 2016 - Evaluating Entity Linking An Analysis of Current Benchmark Datasets and a Roadmap for Doing a Better Job.pdf:pdf},
isbn = {9782951740891},
keywords = {benchmark,entity linking,evaluation},
pages = {4373--4379},
title = {{Evaluating Entity Linking: An Analysis of Current Benchmark Datasets and a Roadmap for Doing a Better Job}},
url = {http://www.eurecom.fr/fr/publication/4859/download/ds-publi-4859.pdf},
year = {2016}
}
@article{Hachey2015,
abstract = {We describe the SYDNEY submission to the TAC 2015 event nugget detection shared task. Development experiments explore the contri-bution of features aimed at improving gen-eralisation. They indicate that Brown clus-ter, Nomlex and WordNet features are comple-mentary with more improvement from Word-Net features. Final submissions differ in the number of negative examples ued for training trigger detection, with 10{\%} subsampling re-sulting in our best f-score of 51.97.},
author = {Hachey, Ben},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Hachey - 2015 - Overview of SYDNEY System for TAC KBP 2015 Event Nugget Detection.pdf:pdf},
pages = {2--5},
title = {{Overview of SYDNEY System for TAC KBP 2015 Event Nugget Detection}},
year = {2015}
}
@article{Wick2008,
abstract = {The automatic consolidation of database records from many heterogeneous sources into a single repository requires solv- ing several information integration tasks. Although tasks such as coreference, schema matching, and canonicalization are closely related, they are most commonly studied in iso- lation. Systems that do tackle multiple integration prob- lems traditionally solve each independently, allowing errors to propagate from one task to another. In this paper, we de- scribe a discriminatively-trained model that reasons about schema matching, coreference, and canonicalization jointly. We evaluate ourmodel on a real-world data set of people and demonstrate that simultaneously solving these tasks reduces errors over a cascaded or isolated approach. Our experi- ments show that a joint model is able to improve substan- tially over systems that either solve each task in isolation or with the conventional cascade. We demonstrate nearly a 50{\%} error reduction for coreference and a 40{\%} error reduc- tion for schema matching.},
author = {Wick, Michael L. and Rohanimanesh, Khashayar and Schultz, Karl and McCallum, Andrew},
doi = {10.1145/1401890.1401977},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Wick et al. - 2008 - A unified approach for schema matching, coreference and canonicalization.pdf:pdf},
isbn = {9781605581934},
journal = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08},
keywords = {all or part of,calization,canoni-,conditional random field,coreference,data integration,or hard copies of,permission to make digital,schema matching,this work for,weighted logic},
pages = {722},
title = {{A unified approach for schema matching, coreference and canonicalization}},
url = {http://dl.acm.org/citation.cfm?doid=1401890.1401977},
year = {2008}
}
@article{Chesney,
author = {Chesney, Sophie and Jacquet, Guillaume and Steinberger, Ralf and Piskorski, Jakub},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chesney et al. - 2017 - Multi-word Entity Classification in a Highly Multilingual Environment.pdf:pdf},
journal = {Proceeding of the 13th Workshop on Multiword Expressions (MWE 2017)},
pages = {11--20},
title = {{Multi-word Entity Classification in a Highly Multilingual Environment}},
year = {2017}
}
@article{Zeng2014,
abstract = {The state-of-the-art methods used for relation classification are primarily based on statistical ma-chine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language process-ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings 1 . Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the fea-tures are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.01006v1},
author = {Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
doi = {http://aclweb.org/anthology/C/C14/C14-1220.pdf},
eprint = {arXiv:1508.01006v1},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Zeng et al. - 2014 - Relation Classification via Convolutional Deep Neural Network.pdf:pdf},
isbn = {978-1-941643-26-6},
journal = {Coling},
number = {2011},
pages = {2335--2344},
pmid = {91150},
title = {{Relation Classification via Convolutional Deep Neural Network}},
url = {http://www.nlpr.ia.ac.cn/cip/liukang.files/camera{\_}coling2014{\_}final.pdf},
year = {2014}
}
@article{Lin2012,
author = {Lin, Wen-pin},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lin - 2012 - Rich Annotation Guided Learning.pdf:pdf},
keywords = {-rich annotation,feature engineering},
number = {3},
pages = {261--277},
title = {{Rich Annotation Guided Learning}},
volume = {5},
year = {2012}
}
@article{Hsi2016,
abstract = {This year, the CMU CS Event team partici-pated in the Event Argument Extraction and Linking Task. We utilize a pipeline of classi-fiers approach to event extraction, with partic-ular focus on leveraging resources from mul-tiple languages in order to train a single cross-lingual model. We apply our system to tri-lingual event extraction, resulting in the top performance on both the Chinese and Spanish document-level sub-tasks.},
author = {Hsi, Andrew and Carbonell, Jaime and Yang, Yiming},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Hsi, Carbonell, Yang - 2016 - CMU CS Event TAC-KBP2016 Event Argument Extraction System.pdf:pdf},
keywords = {TAC{\_}KBP{\_}2016{\_}events},
mendeley-tags = {TAC{\_}KBP{\_}2016{\_}events},
title = {{CMU CS Event TAC-KBP2016 Event Argument Extraction System}},
url = {https://tac.nist.gov/publications/2016/participant.papers/TAC2016.CMU{\_}CS{\_}Event.proceedings.pdf},
year = {2016}
}
@article{Valenzuela-Escarcega2015,
abstract = {We describe the design, development, and API of ODIN (Open Domain INformer), a domain-independent, rule-based event extraction (EE) framework. The proposed EE approach is: simple (most events are captured with simple lexico-syntactic patterns), powerful (the lan-guage can capture complex constructs, such as events taking other events as arguments, and regular expressions over syntactic graphs), robust (to recover from syntactic parsing er-rors, syntactic patterns can be freely mixed with surface, token-based patterns), and fast (the runtime environment processes 110 sen-tences/second in a real-world domain with a grammar of over 200 rules). We used this framework to develop a grammar for the bio-chemical domain, which approached human performance. Our EE framework is accom-panied by a web-based user interface for the rapid development of event grammars and vi-sualization of matches. The ODIN framework and the domain-specific grammars are avail-able as open-source code.},
author = {Valenzuela-Esc{\'{a}}rcega, Marco A and Hahn-Powell, Gus and Hicks, Thomas and Surdeanu, Mihai},
doi = {10.3115/v1/P15-4022},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Valenzuela-Esc{\'{a}}rcega et al. - 2015 - A Domain-independent Rule-based Framework for Event Extraction.pdf:pdf},
isbn = {9781941643990},
journal = {Proceedings of ACL-IJCNLP 2015 System Demonstrations},
pages = {127--132},
title = {{A Domain-independent Rule-based Framework for Event Extraction}},
url = {http://aclanthology.info/papers/a-domain-independent-rule-based-framework-for-event-extraction},
year = {2015}
}
@article{Ji2010,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at TAC 2010. The main goal of KBP is to promote research in discovering facts about entities and expanding a structured knowledge base with this information. A large source collection of newswire and web documents is provided for systems to discover information. Attributes (a.k.a. “slots”) derived from Wikipedia infoboxes are used to create the reference knowledge base (KB). KBP2010 includes the following four tasks: (1) Regular Entity Linking, where names must be aligned to entities in the KB; (2) Optional Entity linking, without using Wikipedia texts; (3) Regular Slot Filling, which requires a system to automatically discover the attributes of specified entities from the source document collection and use them to expand the KB; (4) Surprise Slot Filling, which requires a system to return answers regarding new slot types within a short time period. KBP2010 has attracted many participants (over 45 teams registered for KBP 2010 (not including the RTEKBP Validation Pilot task), among which 23 teams submitted results). In this paper we provide an overview of the task definition and annotation challenges associated with KBP2010. Then we summarize the evaluation results and discuss the lessons that we have learned based on detailed analysis.},
author = {Ji, Heng and Grishman, Ralph and Dang, Hoa Trang and Griffitt, Kira and Ellis, Joe},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji et al. - 2010 - Overview of the TAC 2010 Knowledge Base Population Track.pdf:pdf;:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji et al. - 2010 - Overview of the TAC 2010 Knowledge Base Population Track(2).pdf:pdf;:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji et al. - 2010 - Overview of the TAC 2010 Knowledge Base Population Track(3).pdf:pdf},
isbn = {1117458812938},
journal = {Tac 2010},
number = {November},
title = {{Overview of the TAC 2010 Knowledge Base Population Track}},
year = {2010}
}
@unpublished{LDC2016.rich_ere,
author = {{Linguistic Data Consortium}},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Linguistic Data Consortium - 2016 - Rich ERE Annotation Guidelines Overview V4.2.pdf:pdf},
publisher = {Linguistics Data Consortium},
title = {{Rich ERE Annotation Guidelines Overview V4.2}},
year = {2016}
}
@article{Nguyen2016,
abstract = {Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have ei-ther applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and gen-eralized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined ap-proach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby benefiting from the advantages of the two models as well as addressing issues inher-ent in the existing approaches. We systemati-cally investigate different memory features for the joint model and demonstrate that the pro-posed model achieves the state-of-the-art per-formance on the ACE 2005 dataset.},
author = {Nguyen, Thien Huu and Cho, Kyunghyun and Grishman, Ralph},
doi = {10.18653/v1/N16-1034},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Nguyen, Cho, Grishman - 2016 - Joint Event Extraction via Recurrent Neural Networks.pdf:pdf},
isbn = {9781941643914},
journal = {Naacl2016},
pages = {300--309},
title = {{Joint Event Extraction via Recurrent Neural Networks}},
year = {2016}
}
@article{Nguyen2015,
abstract = {We study the event detection problem us-ing convolutional neural networks (CNNs) that overcome the two fundamental limi-tations of the traditional feature-based ap-proaches to this task: complicated feature engineering for rich feature sets and er-ror propagation from the preceding stages which generate these features. The experi-mental results show that the CNNs outper-form the best reported feature-based sys-tems in the general setting as well as the domain adaptation setting without resort-ing to extensive external resources.},
author = {Nguyen, Thien Huu and Grishman, Ralph},
doi = {10.3115/v1/P15-2060},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Nguyen, Grishman - 2015 - Event Detection and Domain Adaptation with Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643730},
journal = {Acl},
pages = {365--371},
title = {{Event Detection and Domain Adaptation with Convolutional Neural Networks}},
url = {http://aclweb.org/anthology/P15-2060},
year = {2015}
}
@inproceedings{Denis2007,
abstract = {Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f-score improvements of 3.7-5.3{\{}{\%}{\}} over a base coreference classifier on the ACE datasets.},
annote = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, NAACL HLT 2007 ; Conference date: 22-04-2007 Through 27-04-2007},
author = {Denis, Pascal and Baldridge, Jason},
pages = {236--243},
title = {{Joint determination of anaphoricity and coreference resolution using integer programming}},
year = {2007}
}
@misc{Meij2013,
author = {Meij, Edgar and Balog, Krisztian and Odijk, Daan},
title = {{Entity linking and retrieval tutorial}},
url = {http://ejmeij.github.io/entity-linking-and-retrieval-tutorial/},
year = {2013}
}
@article{Ji2008,
abstract = {We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “dis- course” from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related doc- uments with local decisions, we design a sim- ple scheme to conduct cross-document inference for improving the ACE event ex- traction task1. Without using any additional labeled data this new approach obtained 7.6{\%} higher F-Measure in trigger labeling and 6{\%} higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence. 1},
author = {Ji, Heng and Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji, Grishman - 2008 - Refining Event Extraction through Cross-document Inference.pdf:pdf},
journal = {Proceedings of ACL-08: HLT},
pages = {254--262},
title = {{Refining Event Extraction through Cross-document Inference}},
year = {2008}
}
@article{Suominen2010,
abstract = {Publishing information about upcoming events such as concerts and discussion$\backslash$ngroup meetings in a structured format allows the event information to be$\backslash$naggregated, filtered and delivered to potential participants. Making$\backslash$nautomatic personalized recommendations about events requires structured$\backslash$nmetadata such as machine-understandable locations and semantic descriptions$\backslash$nabout the topic and audience of the event. We present a survey of the state$\backslash$nof current semantic representation formats for events, including iCalendar$\backslash$nand its RDFa and microformat representations, and show that their$\backslash$nsupport for expressing rich structured metadata is limited. We have also$\backslash$ntested how well different tools support and understand the formats. Based on$\backslash$nthe surveys we have implemented a rich event information schema for a$\backslash$nhealth-oriented activity portal and developed an aggregation and validation$\backslash$ntool for gathering and processing event information.},
author = {Suominen, Osma and Hyv{\"{o}}nen, Eero},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Suominen, Hyv{\"{o}}nen - 2010 - Expressing and aggregating rich event descriptions.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Expressing and aggregating rich event descriptions}},
volume = {699},
year = {2010}
}
@article{Reimers2002,
author = {Reimers, Nils and Gurevych, Iryna},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Reimers, Gurevych - 2015 - Event Nugget Detection , Classification and Coreference Resolution using Deep Neural Networks and Grandient B.pdf:pdf},
title = {{Event Nugget Detection , Classification and Coreference Resolution using Deep Neural Networks and Grandient Boosted Decision Trees}},
year = {2015}
}
@article{Navigli2007,
abstract = {Word sense disambiguation (WSD) has been a long-standing research objective for natural language processing. In this paper we are concerned with developing graph-based unsupervised algorithms for alleviating the data requirements for large scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most "important" node among the set of graph nodes representing its senses. We propose a variety of measures that analyse the connectivity of graph structures, thereby identifying the most relevant word senses. We assess their performance on standard datasets, and show that the best measures perform comparably to state-of-the-art.},
author = {Navigli, Roberto and Lapata, Mirella},
doi = {10.1109/ICIIECS.2015.7193083},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Navigli, Lapata - 2007 - Graph connectivity measures for unsupervised word sense disambiguation.pdf:pdf},
isbn = {9781479968183},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {1683--1688},
title = {{Graph connectivity measures for unsupervised word sense disambiguation}},
year = {2007}
}
@article{Ikuta2014,
abstract = {The goal of this study is to create guide- lines for annotating cause-effect relations as part of the Richer Event Description schema. We present the challenges faced using the definition of causation in terms of counterfactual dependence and propose new guidelines for cause-effect annotation using an alternative definition which treats causation as an intrinsic relation between events. To support the use of such an in- trinsic definition, we examine the theoret- ical problems that the counterfactual def- inition faces, show how the intrinsic defi- nition solves those problems, and explain how the intrinsic definition adheres to psy- chological reality, at least for our annota- tion purposes, better than the counterfac- tual definition. We then evaluate the new guidelines by presenting results obtained from pilot annotations of ten documents, showing that an inter-annotator agreement (F1-score) of 0.5753 was achieved. The results provide a benchmark for future studies concerning cause-effect annotation in the RED schema},
author = {Ikuta, Rei and Iv, William F Styler and Hamang, Mariah and Gorman, Tim O and Palmer, Martha and Styler, Will and O'Gorman, Tim},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ikuta et al. - 2014 - Challenges of Adding Causation to Richer Event Descriptions.pdf:pdf},
journal = {Proceedings of the Second Workshop on EVENTS: Definition, Detection, Coreference, and Representation},
pages = {12--20},
title = {{Challenges of Adding Causation to Richer Event Descriptions}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2903},
year = {2014}
}
@article{Li2015,
abstract = {Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. But there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper we benchmark {\{}$\backslash$bf recursive{\}} neural models against sequential {\{}$\backslash$bf recurrent{\}} neural models (simple recurrent and LSTM models), enforcing apples-to-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answer-phrases; (3) discourse parsing; (4) semantic relation extraction (e.g., {\{}$\backslash$em component-whole{\}} between nouns). Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require associating headwords across a long distance, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.},
archivePrefix = {arXiv},
arxivId = {1503.00185},
author = {Li, Jiwei and Luong, Minh-Thang and Jurafsky, Dan and Hovy, Eduard},
doi = {10.18653/v1/D15-1278},
eprint = {1503.00185},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li et al. - 2015 - When Are Tree Structures Necessary for Deep Learning of Representations.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {2304--2314},
title = {{When Are Tree Structures Necessary for Deep Learning of Representations?}},
url = {http://arxiv.org/abs/1503.00185},
year = {2015}
}
@article{Vossen2017,
abstract = {In this paper we describe a method to detect event descrip-tions in different news articles and to model the semantics of events and their components using RDF representations. We compare these descriptions to solve a cross-document event coreference task. Our com-ponent approach to event semantics defines identity and granularity of events at different levels. It performs close to state-of-the-art approaches on the cross-document event coreference task, while outperforming other works when assuming similar quality of event detection. We demonstrate how granularity and identity are interconnected and we discuss how se-mantic anomaly could be used to define differences between coreference, subevent and topical relations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1704.04259v1},
author = {Vossen, Piek and Cybulska, Agata},
eprint = {arXiv:1704.04259v1},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Vossen, Cybulska - 2017 - Identity and Granularity of Events in Text.pdf:pdf},
keywords = {()},
title = {{Identity and Granularity of Events in Text}},
url = {http://cltl.nl},
year = {2017}
}
@article{Finkel2005,
abstract = {Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, andCRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9{\%} over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Finkel, Grenager, Manning - 2005 - Incorporating non-local information into information extraction systems by gibbs sampling.pdf:pdf},
issn = {02773791},
journal = {in Acl},
number = {1995},
pages = {363 -- 370},
title = {{Incorporating non-local information into information extraction systems by gibbs sampling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.8904},
year = {2005}
}
@article{Burel2017,
abstract = {In this paper, we introduce Dual-CNN, a semantically-enhanced deep learning model to target the problem of event detection in crisis situations from social media data. A layer of semantics is added to a traditional Convolutional Neural Network (CNN) model to capture the contextual information that is gen-erally scarce in short, ill-formed social media messages. Our results show that our methods are able to successfully identify the existence of events, and event types (hurricane, floods, etc.) accurately ({\textgreater} 79{\%} F-measure), but the performance of the model significantly drops (61{\%} F-measure) when identifying fine-grained event-related information (affected individuals, damaged infrastructures, etc.). These results are competitive with more traditional Machine Learning models, such as SVM.},
author = {Burel, Gr{\'{e}}goire and Saif, Hassan and Fernandez, Miriam and Alani, Harith},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Burel et al. - 2017 - On Semantics and Deep Learning for Event Detection in Crisis Situations.pdf:pdf},
journal = {Workshop on Semantic Deep Learning (SemDeep), at ESWC 201},
keywords = {CNN,Dual-CNN,Event Detection,Se-mantic Embeddings,Semantic Deep Learning,Word Embeddings},
title = {{On Semantics and Deep Learning for Event Detection in Crisis Situations}},
year = {2017}
}
@article{Song2017,
abstract = {Machine learning has become pervasive in multiple domains, impacting a wide variety of applications, such as knowledge discovery and data mining, natural language processing, information retrieval, computer vision, social and health informatics, ubiquitous computing, etc. Two essential problems of machine learning are how to generate features and how to acquire labels for machines to learn. Particularly, labeling large amount of data for each domain-specific problem can be very time consuming and costly. It has become a key obstacle in making learning protocols realistic in applications. In this paper, we will discuss how to use the existing general-purpose world knowledge to enhance machine learning processes, by enriching the features or reducing the labeling work. We start from the comparison of world knowledge with domain-specific knowledge, and then introduce three key problems in using world knowledge in learning processes, i.e., explicit and implicit feature representation, inference for knowledge linking and disambiguation, and learning with direct or indirect supervision. Finally we discuss the future directions of this research topic.},
archivePrefix = {arXiv},
arxivId = {1705.02908},
author = {Song, Yangqiu and Roth, Dan},
eprint = {1705.02908},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Song, Roth - 2017 - Machine Learning with World Knowledge The Position and Survey.pdf:pdf},
pages = {1--20},
title = {{Machine Learning with World Knowledge: The Position and Survey}},
url = {http://arxiv.org/abs/1705.02908},
year = {2017}
}
@misc{Consortium2008b,
abstract = {The Entity Detection task requires that selected types of entities mentioned in the source data be detected, their sense disambiguated, and that selected attributes of these entities be extracted and merged into a unified representation for each entity.},
booktitle = {Facilities},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Unknown - 2008 - ACE English Annotation Guidelines for Relations (v6.2).pdf:pdf},
pages = {72},
publisher = {Linguistics Data Consortium},
title = {{ACE English Annotation Guidelines for Relations (v6.2)}},
url = {https://www.ldc.upenn.edu/collaborations/past-projects/ace/annotation-tasks-and-specifications},
year = {2008}
}
@misc{Vossen2016a,
author = {Vossen, Piek},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Vossen - 2016 - Newsreader public summary.pdf:pdf},
pages = {1--29},
title = {{Newsreader public summary}},
volume = {31},
year = {2016}
}
@article{Tsai2016,
abstract = {Named Entity Recognition (NER) mod-els for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on an-other, source, language (or multiple source languages). We introduce a language independent method for NER, building on cross-lingual wikification, a technique that grounds words and phrases in non-English text into English Wikipedia en-tries. Thus, mentions in any language can be described using a set of cate-gories and FreeBase types, yielding, as we show, strong language-independent fea-tures. With this insight, we propose an NER model that can be applied to all lan-guages in Wikipedia. When trained on English, our model outperforms compa-rable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on low-resource languages (e.g., Turkish, Taga-log, Yoruba, Bengali, and Tamil) that have significantly smaller Wikipedia. More-over, our method allows us to train on mul-tiple source languages, typically improv-ing NER results on the target languages. Finally, we show that our language-independent features can be used also to enhance monolingual NER systems, yield-ing improved results for all 9 languages.},
author = {Tsai, Chen-tse and Mayhew, Stephen and Roth, Dan and Goodwin, N.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Tsai et al. - 2016 - Cross-Lingual Named Entity Recognition via Wikification.pdf:pdf},
pages = {219--228},
title = {{Cross-Lingual Named Entity Recognition via Wikification}},
year = {2016}
}
@article{White2006,
abstract = {Individual causal relations tend to form parts of larger causal systems. In five experiments the ability of participants to infer the structures of two systems involving five entities from patterns of cooccurrence was investigated. Although the systems were fully deterministic, there were no indirect causal relations, and participants were given guidance on how to infer causal structure from cooccurrence information, low rates of success were observed. Judgements were based more on information about temporal relations, even though no guidance on the use of temporal relation information for causal inference was provided. When no guidance was provided, no participants succeeded in inferring the structure of both systems. The results indicate that temporal relations may be preferred as cues to causal structure over patterns of cooccurrence.},
author = {White, Peter A.},
doi = {10.1080/09541440500264861},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/White - 2006 - How well is causal structure inferred from cooccurrence information.pdf:pdf},
issn = {09541446},
journal = {European Journal of Cognitive Psychology},
number = {3},
pages = {454--480},
title = {{How well is causal structure inferred from cooccurrence information?}},
volume = {18},
year = {2006}
}
@article{Lever2016,
abstract = {We present the Vancouver Event and Re- lation System for Extraction (VERSE)1 as a competing system for three subtasks of the BioNLP Shared Task 2016. VERSE performs full event extraction including entity, relation and modification extrac- tion using a feature-based approach. It achieved the highest F1-score in the Bac- teria Biotope (BB3) event subtask and the third highest F1-score in the Seed Devel- opment (SeeDev) binary subtask.},
author = {Lever, Jake and Jones, Steven J. M.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lever, Jones - 2016 - VERSE Event and Relation Extraction in the BioNLP 2016 Shared Task.pdf:pdf},
journal = {Proceedings of the 4th BioNLP Shared Task Workshop},
pages = {42--49},
title = {{VERSE : Event and Relation Extraction in the BioNLP 2016 Shared Task}},
year = {2016}
}
@article{Yangarber2005,
author = {Yangarber, Roman and Jokipii, Lauri},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yangarber, Jokipii - 2005 - Redundancy-based Correction of Automatically Extracted Facts.pdf:pdf},
journal = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
number = {October},
pages = {57--64},
title = {{Redundancy-based Correction of Automatically Extracted Facts}},
url = {http://www.aclweb.org/anthology/H/H05/H05-1008},
year = {2005}
}
@book{Jurafsky2017,
abstract = {Third Edition draft},
author = {Jurafsky, Daniel and Martin, James H},
doi = {10.1162/089120100750105975},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Jurafsky, Martin - 2017 - Speech and Language Processing - An Introduction to Natural Language Processing, Computational Linguistics, an.pdf:pdf},
isbn = {978-0131873216},
issn = {08912017},
title = {{Speech and Language Processing - An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (Third Edition Draft)}},
url = {https://web.stanford.edu/{~}jurafsky/slp3/ed3book.pdf},
year = {2017}
}
@incollection{Jurafsky2017.Information_Extraction,
abstract = {Imagine that you are an analyst with an investment firm that tracks airline stocks. You're given the task of determining the relationship (if any) between airline an-nouncements of fare increases and the behavior of their stocks the next day. His-torical data about stock prices is easy to come by, but what about the airline an-nouncements? You will need to know at least the name of the airline, the nature of the proposed fare hike, the dates of the announcement, and possibly the response of other airlines. Fortunately, these can be all found in news articles like this one: Citing high fuel prices, United Airlines said Friday it has increased fares by {\$}6 per round trip on flights to some cities also served by lower-cost carriers. American Airlines, a unit of AMR Corp., immediately matched the move, spokesman Tim Wagner said. United, a unit of UAL Corp., said the increase took effect Thursday and applies to most routes where it competes against discount carriers, such as Chicago to Dallas and Denver to San Francisco. This chapter presents techniques for extracting limited kinds of semantic con-tent from text. This process of information extraction (IE), turns the unstructured},
author = {Jurafsky, Daniel and Martin, James H},
booktitle = {Speech and Language Processing},
chapter = {21},
doi = {10.1145/234173.234209},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Jurafsky, Martin - 2017 - Information extraction.pdf:pdf},
isbn = {9781601981882},
issn = {19317883},
number = {August},
pages = {31},
pmid = {8806820},
title = {{Information extraction}},
url = {http://dl.acm.org/citation.cfm?id=234209},
year = {2017}
}
@article{Sarawagi2004,
abstract = {We describe semi-Markov conditional random fields (semi-CRFs), a conditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a “segmentation ” of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outperform conventional CRFs.},
author = {Sarawagi, Sunita and Cohen, W.},
doi = {10.1.1.128.3524},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Sarawagi, Cohen - 2004 - Semi-markov conditional random fields for information extraction.pdf:pdf},
issn = {{\textless}null{\textgreater}},
journal = {Advances in Neural Information Processing Systems 17},
pages = {1185----1192},
title = {{Semi-markov conditional random fields for information extraction}},
url = {http://work-tmp.googlecode.com/svn/trunk/CRF/10.1.1.128.3524.pdf},
year = {2004}
}
@article{Lu2015a,
author = {Lu, Jing and Ng, Vincent},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lu, Ng - 2015 - Event Coreference Resolution with Multi-Pass Sieves.pdf:pdf},
isbn = {9782951740891},
keywords = {discourse processing,event coreference resolution,information extraction},
pages = {3996--4003},
title = {{Event Coreference Resolution with Multi-Pass Sieves}},
year = {2015}
}
@article{Lafferty2001,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.4088v1},
author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando C N},
doi = {10.1038/nprot.2006.61},
eprint = {arXiv:1011.4088v1},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lafferty, McCallum, Pereira - 2001 - Conditional random fields Probabilistic models for segmenting and labeling sequence data.pdf:pdf},
isbn = {1558607781},
issn = {1750-2799},
journal = {ICML '01 Proceedings of the Eighteenth International Conference on Machine Learning},
keywords = {Animals,Antigen,Gene Transfer Techniques,Genetic Engineering,Genetic Engineering: methods,Genetic Vectors,Genetic Vectors: metabolism,Mice,Receptors,Retroviridae,Retroviridae: genetics,Stem Cells,Stem Cells: metabolism,T-Cell,Transgenic,Transgenic: genetics,Transgenic: metabolism,alpha-beta,alpha-beta: genetics,alpha-beta: metabolism},
number = {June},
pages = {282--289},
pmid = {17406263},
title = {{Conditional random fields: Probabilistic models for segmenting and labeling sequence data}},
url = {http://repository.upenn.edu/cis{\_}papers/159/{\%}5Cnhttp://dl.acm.org/citation.cfm?id=655813},
volume = {8},
year = {2001}
}
@article{Li2014.ije-om-r,
abstract = {We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search. A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a number of new and effective global features as soft constraints to capture the inter-dependency among entity mentions and relations. Experiments on Automatic Content Extraction (ACE) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system.},
author = {Li, Qi and Ji, Heng},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li, Ji - 2014 - Incremental Joint Extraction of Entity Mentions and Relations.pdf:pdf},
isbn = {9781937284725},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)},
pages = {402--412},
title = {{Incremental Joint Extraction of Entity Mentions and Relations}},
url = {https://aclweb.org/anthology/P/P14/P14-1038.pdf},
year = {2014}
}
@article{Poon2008,
abstract = {Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.},
author = {Poon, Hoifung and Domingos, Pedro},
doi = {10.3115/1613715.1613796},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Poon, Domingos - 2008 - Joint Unsupervised Coreference Resolution with Markov logic.pdf:pdf},
journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
number = {October},
pages = {650},
title = {{Joint Unsupervised Coreference Resolution with Markov logic}},
url = {http://portal.acm.org/citation.cfm?doid=1613715.1613796},
year = {2008}
}
@article{Raghunathan2010,
abstract = {Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier's entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sieve-based approaches could be applied to other NLP tasks.},
author = {Raghunathan, Karthik and Lee, Heeyoung and Rangarajan, Sudarshan and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan and Manning, Christopher D.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Raghunathan et al. - 2010 - A Multi-Pass Sieve for Coreference Resolution.pdf:pdf},
isbn = {1932432868},
journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP '10)},
number = {October},
pages = {492--501},
title = {{A Multi-Pass Sieve for Coreference Resolution}},
url = {http://dl.acm.org/citation.cfm?id=1870706},
year = {2010}
}
@article{Tang2015,
abstract = {Document level sentiment classification remains a challenge: encoding the intrin- sic relations between sentences in the se- mantic meaning of a document. To ad- dress this, we introduce a neural network model to learn vector-based document rep- resentation in a unified, bottom-up fash- ion. The model first learns sentence rep- resentation with convolutional neural net- work or long short-term memory. After- wards, semantics of sentences and their relations are adaptively encoded in docu- ment representation with gated recurren- t neural network. We conduct documen- t level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimen- tal results show that: (1) our neural mod- el shows superior performances over sev- eral state-of-the-art algorithms; (2) gat- ed recurrent neural network dramatically outperforms standard recurrent neural net- work in document modeling for sentiment classification},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Tang, Duyu and Qin, Bing and Liu, Ting},
doi = {10.18653/v1/D15-1167},
eprint = {1508.04025},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Tang, Qin, Liu - 2015 - Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
number = {September},
pages = {1422--1432},
title = {{Document Modeling with Gated Recurrent Neural Network for Sentiment Classification}},
url = {http://aclweb.org/anthology/D15-1167},
year = {2015}
}
@article{Griffitt2016,
abstract = {Changes from V1.0  Added reference to " Annotation Guidelines for Individuality of Specific Entities: DEFT Entities, Relations, Events (ERE) " under Individuality section (3.2)  Revised instructions under 3.7.2 (ORGs) and 3.7.3 (GPEs) to match ERE on nominal top-level government mentions, to tag them as separate, distinct ORG mentions of the governments. Changes from 2015:  Section 3.2: Expanded language and examples for determining individual mentions for all entity types.  Section 3.1: Nominal heads (NOM) to be tagged across all languages and entity types. Changed section name to " Mention Types: Name and Nominal " and created subsections for Named and Nominal mention types. The former absorbs the earlier subsection 3.8.1.1 on post authors (as named mentions), and latter absorbs the earlier subsection 3.8.1.2 on PER nominal mentions.  Changes to reflect that all five entity types (FAC, GPE, LOC, ORG, PER) will be annotated across all three languages.  Removed title entity type (TTL) section, retained comments on differences with NOM  Removed distinctions between " full " ED and simplified ED task for Cold Start, as the latter is not active in 2016  Removed Section 3.7 Embedded (intra-token) Mentions, not valid for 2016  Revised Sections 3.2 and 3.3 on individuality and specificity of mentions to reflect consistency with 2016 Rich ERE annotation  Wording changed to reflect that the 2016 Entity Discovery stage acts as an additional pass over ERE-generated Entity annotation  Additional NOM examples},
author = {Griffitt, Kira and Li, Xuansong and Indacochea, Alonso and Mott, Justin},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Griffitt et al. - 2016 - TAC KBP 2016 –Entity Discovery and Linking (EDL) Guidelines Linguistic Data Consortium.pdf:pdf},
title = {{TAC KBP 2016 –Entity Discovery and Linking (EDL) Guidelines Linguistic Data Consortium}},
url = {http://projects.ldc.upenn.edu/kbp/},
year = {2016}
}
@article{Steinberger2015,
author = {Steinberger, Josef and Tanev, Hristo},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Steinberger, Tanev - 2015 - Towards Multilingual Event Extraction Evaluation A Case Study for the Czech Language.pdf:pdf},
issn = {13138502},
journal = {Proceedings of the International Conference Recent Advances in Natural Language Processing},
pages = {627--635},
title = {{Towards Multilingual Event Extraction Evaluation: A Case Study for the Czech Language}},
url = {http://aclweb.org/anthology/R15-1081},
year = {2015}
}
@article{Liu2015,
abstract = {Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.},
archivePrefix = {arXiv},
arxivId = {1507.04646},
author = {Liu, Yang and Wei, Furu and Li, Sujian and Ji, Heng and Zhou, Ming and Wang, Houfeng},
doi = {10.3115/v1/P15-2047},
eprint = {1507.04646},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Liu et al. - 2015 - A Dependency-Based Neural Network for Relation Classification.pdf:pdf},
isbn = {9781941643730},
number = {2006},
title = {{A Dependency-Based Neural Network for Relation Classification}},
url = {http://arxiv.org/abs/1507.04646},
year = {2015}
}
@misc{Zhanga,
archivePrefix = {arXiv},
arxivId = {1801.07883},
author = {Zhang, Lei and Wang, Shuai and Liu, Bing},
eprint = {1801.07883},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Zhang, Wang, Liu - 2017 - Deep Learning for Sentiment Analysis A Survey.pdf:pdf},
title = {{Deep Learning for Sentiment Analysis: A Survey}},
year = {2017}
}
@article{Chiu2015,
abstract = {Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.},
archivePrefix = {arXiv},
arxivId = {1511.08308},
author = {Chiu, Jason P. C. and Nichols, Eric},
eprint = {1511.08308},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Chiu, Nichols - 2015 - Named Entity Recognition with Bidirectional LSTM-CNNs.pdf:pdf},
issn = {2307-387X},
number = {2003},
pages = {357--370},
title = {{Named Entity Recognition with Bidirectional LSTM-CNNs}},
url = {http://arxiv.org/abs/1511.08308},
volume = {4},
year = {2015}
}
@article{VandeKauter2013,
abstract = {This paper presents the LeTs Preprocess Toolkit, a suite of robust high-performance preprocessing modules including Part-of-Speech Taggers, Lemmatizers and Named Entity Recognizers. The currently supported languages are Dutch, English, French and German. We give a detailed description of the architecture of the LeTs Preprocess pipeline and describe the data and methods used to train each component. Ten-fold cross-validation results are also presented. To assess the performance of each module on different domains, we collected real-world textual data from companies covering various domains (a.o. automotive, dredging and human resources) for all four supported languages. For this multi-domain corpus, a manually verified gold standard was created for each of the three preprocessing steps. We present the performance of our preprocessing components on this corpus and compare it to the performance of other existing tools.},
author = {van de Kauter, Marjan and Coorman, Geert and Lefever, Els and Desmet, Bart and Macken, Lieve and Hoste, Veronique},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/van de Kauter et al. - 2013 - LeTs Preprocess The multilingual LT3 linguistic preprocessing toolkit.pdf:pdf},
issn = {2211-4009},
journal = {Computational Linguistics in the Netherlands Journal},
month = {mar},
pages = {103--120},
title = {{LeTs Preprocess: The multilingual LT3 linguistic preprocessing toolkit}},
volume = {3},
year = {2013}
}
@techreport{grishman2005,
author = {Grishman, Ralph and Westbrook, David and Meyers, Adam},
institution = {Department of Computer Science, New York University},
keywords = {GLARF NomBank PropBank dataset{\_}ACE2005 event{\_}extra},
title = {{NYU's English ACE 2005 System Description}},
url = {http://nlp.cs.nyu.edu/publication/papers/ACE05-NYUEnglishSysDescrDec10.pdf},
year = {2005}
}
@misc{www:conll,
title = {{CoNLL: The SIGNLL Conference on Computational Natural Language Learning}},
url = {http://www.conll.org/2018}
}
@article{Francis-Landau2016,
abstract = {A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).},
archivePrefix = {arXiv},
arxivId = {1604.00734},
author = {Francis-Landau, Matthew and Durrett, Greg and Klein, Dan},
eprint = {1604.00734},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Francis-Landau, Durrett, Klein - 2016 - Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks.pdf:pdf},
isbn = {9781941643914},
journal = {Annual Conference of the North American Chapter of the Association for Computational Linguistics},
title = {{Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1604.00734},
year = {2016}
}
@misc{Ji2014,
author = {Ji, Heng},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji - 2014 - A presentation on Relation Extraction and Event Extraction.pdf:pdf},
title = {{A presentation on Relation Extraction and Event Extraction}},
url = {http://projects.ldc.upenn.edu/ace/docs/English-Entities-Guidelines{\_}v6.6.pdf},
year = {2014}
}
@article{Nguyen2016a,
abstract = {This is the first time New York University (NYU) participates in the event nugget (EN) evaluation of the Text Analysis Conference (TAC). We developed EN systems for both subtasks of event nugget, i.e, EN Task 1: Event Nugget Detection and EN Task 2: Event Nugget Detection and Coreference. The sys-tems are mainly based on our recent research on deep learning for event detection (Nguyen and Grishman, 2015a; Nguyen and Grishman, 2016a). Due to the limited time we could de-vote to system development this year, we only ran the systems on the English evaluation data. However, we expect that the adaptation of the current systems to new languages can be done quickly. The development experiments show that although our current systems do not rely on complicated feature engineering, they sig-nificantly outperform the reported systems last year for the EN subtasks on the 2015 evalua-tion data.},
author = {Nguyen, Thien Huu and Meyers, Adam and Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Nguyen, Meyers, Grishman - 2016 - New York University 2016 System for KBP Event Nugget A Deep Learning Approach.pdf:pdf},
journal = {Trec Kbp},
title = {{New York University 2016 System for KBP Event Nugget : A Deep Learning Approach}},
url = {https://cs.nyu.edu/{~}thien/pubs/tac2016.pdf},
year = {2016}
}
@inproceedings{Segers,
abstract = {This paper presents the Event and Implied Situation Ontology (ESO), a manually constructed resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology is built on top of existing resources such as WordNet, SUMO and FrameNet. The ontology is injected to the Predicate Matrix, a resource that integrates predicate and role information from amongst others FrameNet, VerbNet, PropBank, NomBank and WordNet. We illustrate how these resources are used on large document collections to detect information that otherwise would have remained implicit. The ontology is evaluated on two aspects: recall and precision based on a manually annotated corpus and secondly, on the quality of the knowledge inferred by the situation assertions in the ontology. Evaluation results on the quality of the system show that 50{\%} of the events typed and enriched with ESO assertions are correct.},
author = {Segers, Roxane and Rospocher, Marco and Vossen, Piek and Laparra, Egoitz and Rigau, German and Minard, Anne-Lyse},
booktitle = {LREC},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Segers et al. - 2016 - The Event and Implied Situation Ontology (ESO) Application and Evaluation.pdf:pdf},
isbn = {9782951740891},
keywords = {ontology,semantic role labeling,semantic web,text mining},
pages = {1463--1470},
title = {{The Event and Implied Situation Ontology (ESO): Application and Evaluation}},
year = {2016}
}
@article{agerri:jai,
author = {Agerri, Rodrigo and Rigau, German},
journal = {Journal of Artificial Intelligence},
title = {{Robust Multilingual Named Entity Recognition with Shallow Semi-supervised Features}}
}
@article{Pivovarova2017,
abstract = {Task 5 of SemEval-2017 involves fine-grained sentiment analysis on financial
microblogs and news. Our solution for determining the sentiment score extends
an earlier convolutional neural network for sentiment analysis in several ways.
We explicitly encode a focus on a particular company, we apply a data
augmentation scheme, and use a larger data collection to complement the small
training data provided by the task organizers. The best
results
were
achieved
by training a model on an external dataset and then tuning it using the
provided training dataset.},
author = {Pivovarova, Lidia and Escoter, Lloren{\c{c}} and Klami, Arto and Yangarber, Roman},
doi = {10.18653/V1/S17-2143},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Pivovarova et al. - 2017 - HCS at SemEval-2017 Task 5 Polarity detection in business news using convolutional neural networks.pdf:pdf},
journal = {Proceedings of the 11th International Workshop on Semantic Evaluation      (SemEval-2017)},
pages = {842--846},
title = {{HCS at SemEval-2017 Task 5: Polarity detection in business news using      convolutional neural networks}},
url = {https://aclanthology.coli.uni-saarland.de/papers/S17-2143/s17-2143},
year = {2017}
}
@book{JurafskyMartin2009.SLP_2ndEd,
address = {Upper Saddle River, NJ, USA},
author = {Jurafsky, Daniel and Martin, James H},
isbn = {0131873210},
publisher = {Prentice-Hall, Inc.},
title = {{Speech and Language Processing (2nd Edition)}},
year = {2009}
}
@article{Deng2015,
author = {Deng, Jingsheng and Qiao, Fengcai and Li, Hongying and Zhang, Xin and Wang, Hui},
doi = {10.1109/CyberC.2015.24},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Deng et al. - 2015 - An overview of event extraction from twitter.pdf:pdf},
isbn = {9781467391993},
journal = {Proceedings - 2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery, CyberC 2015},
keywords = {event categorization,event extraction,event message identification,semantic elements extraction},
pages = {251--256},
title = {{An overview of event extraction from twitter}},
year = {2015}
}
@article{Huang2012,
abstract = {Most event extraction systems are trained with supervised learning and rely on a col- lection of annotated documents. Due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping so- lution for event role filler extraction that re- quiresminimal human supervision.We aim to rapidly train a state-of-the-art event ex- traction system using a small set of “seed nouns” for each event role, a collection of relevant (in-domain) and irrelevant (out- of-domain) texts, and a semantic dictio- nary. The experimental results show that the bootstrapped systemoutperforms previ- ousweakly supervised event extraction sys- tems on the MUC-4 data set, and achieves performance levels comparable to super- vised training with 700 manually annotated documents.},
author = {Huang, Ruihong and Riloff, Ellen},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Huang, Riloff - 2012 - （已看）Bootstrapped Training of Event Extraction Classifiers.pdf:pdf},
isbn = {9781937284190},
journal = {Eacl2012},
number = {Eacl},
pages = {286--295},
title = {{（已看）Bootstrapped Training of Event Extraction Classifiers}},
year = {2012}
}
@article{Yang2016b,
author = {Yang, Qifan and Huang, Yuzhong and Hao, Leiguang and Tang, Siliang},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yang et al. - 2016 - The ijk System for EAL at TAC KBP 2016 Event Track.pdf:pdf},
title = {{The ijk System for EAL at TAC KBP 2016 Event Track}},
year = {2016}
}
@article{Gale1992,
abstract = {It is well-known that there are polysemous words like sentence whose "meaning" or "sense" depends on the context of use. We have recently reported on two new word-sense disambiguation systems, one trained on bilin- gual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). As this work was nearing com- pletion, we observed a very strong discourse effect. That is, if a polysemous word such as sentence appears two or more times in a well-written discourse, it is extremely likely that they will all share the same sense. This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98{\%}). This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algo- rithm. In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint.},
author = {Gale, William A. and Church, Kenneth W. and Yarowsky, David},
doi = {10.3115/1075527.1075579},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Gale, Church, Yarowsky - 1992 - One sense per discourse.pdf:pdf},
isbn = {1558602720},
journal = {Proceedings of the workshop on Speech and Natural Language  - HLT '91},
pages = {233},
title = {{One sense per discourse}},
url = {http://portal.acm.org/citation.cfm?doid=1075527.1075579},
year = {1992}
}
@article{McClosky2011,
abstract = {ested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a "crime" event can cause a "investigation" event, which can lead to an "arrest" event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP'09 shared task with a F1 score of 53.5{\%} in development and 48.6{\%} in testing.},
author = {McClosky, David and Surdeanu, Mihai and Manning, Christopher D.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/McClosky, Surdeanu, Manning - 2011 - Event extraction as dependency parsing.pdf:pdf},
isbn = {9781932432879},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
keywords = {Event Extraction,Event extraction,dependency parsing},
pages = {1626--1635},
title = {{Event extraction as dependency parsing}},
url = {http://acl.eldoc.ub.rug.nl/mirror/P/P11/P11-1163.pdf{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2002472.2002667},
volume = {1},
year = {2011}
}
@article{Zhang2015,
abstract = {Most approaches to relation extraction, the task of extracting ground facts from natural language text, are based on machine learning and thus starved by scarce training data. Man-ual annotation is too expensive to scale to a comprehensive set of relations. Distant super-vision, which automatically creates training data, only works with relations that already populate a knowledge base (KB). Unfortu-nately, KBs such as FreeBase rarely cover event relations (e.g. " person travels to loca-tion "). Thus, the problem of extracting a wide range of events — e.g., from news streams — is an important, open challenge. This paper introduces NEWSSPIKE-RE, a novel, unsupervised algorithm that discovers event relations and then learns to extract them. NEWSSPIKE-RE uses a novel probabilistic graphical model to cluster sentences describ-ing similar events from parallel news streams. These clusters then comprise training data for the extractor. Our evaluation shows that NEWSSPIKE-RE generates high quality train-ing sentences and learns extractors that per-form much better than rival approaches, more than doubling the area under a precision-recall curve compared to Universal Schemas.},
author = {Zhang, Congle and Soderland, Stephen and Weld, Daniel S.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Zhang, Soderland, Weld - 2015 - Exploiting Parallel News Streams for Unsupervised Event Extraction.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the ACL},
pages = {117--129},
title = {{Exploiting Parallel News Streams for Unsupervised Event Extraction}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/476},
volume = {3},
year = {2015}
}
@article{Hogenboom2016,
author = {Hogenboom, Frederik and Frasincar, Flavius and Kaymak, Uzay and Jong, Franciska De and Caron, Emiel},
doi = {10.1016/j.dss.2016.02.006},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Hogenboom et al. - 2016 - A Survey of event extraction methods from text for decision support systems.pdf:pdf},
keywords = {Event extraction,Information extraction,Natural language processing (NLP),Text mining},
number = {0},
pages = {12--22},
title = {{A Survey of event extraction methods from text for decision support systems}},
volume = {85},
year = {2016}
}
@inproceedings{VanSon,
abstract = {This paper presents a framework and methodology for the annotation of perspectives in text. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives. We propose an annotation scheme that integrates these different phenomena. We use a multilayered annotation approach, splitting the annotation of different aspects of perspectives into small subsequent subtasks in order to reduce the complexity of the task and to better monitor interactions between layers. Currently, we have included four layers of perspective annotation: events, attribution, factuality and opinion. The annotations are integrated in a formal model called GRaSP, which provides the means to represent instances (e.g. events, entities) and propositions in the (real or assumed) world in relation to their mentions in text. Then, the relation between the source and target of a perspective is characterized by means of perspective annotations. This enables us to place alternative perspectives on the same entity, event or proposition next to each other.},
author = {{Van Son}, Chantal and Caselli, Tommaso and Fokkens, Antske and Maks, Isa and Morante, Roser and Aroyo, Lora and Vossen, Piek},
booktitle = {LREC},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Van Son et al. - 2016 - GRaSP A Multilayered Annotation Scheme for Perspectives.pdf:pdf},
isbn = {9782951740891},
keywords = {attribution,multilayered annotation,source perspective},
pages = {1177--1184},
title = {{GRaSP: A Multilayered Annotation Scheme for Perspectives}},
year = {2016}
}
@article{Huang2012a,
abstract = {Most existing theory of structured prediction assumes exact inference, which is often in- tractable in many practical problems. This leads to the routine use of approximate infer- ence such as beam search but there is not much theory behind it. Based on the structured perceptron, we propose a general framework of “violation-fixing” perceptrons for inexact search with a theoretical guarantee for conver- gence under new separability conditions. This framework subsumes and justifies the pop- ular heuristic “early-update” for perceptron with beam search (Collins and Roark, 2004). We also propose several new update meth- ods within this framework, among which the “max-violation” method dramatically reduces training time (by 3 fold as compared to early- update) on state-of-the-art part-of-speech tag- ging and incremental parsing systems.},
author = {Huang, Liang and Fayong, Suphan and Guo, Yang},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Huang, Fayong, Guo - 2012 - Structured perceptron with inexact search.pdf:pdf},
isbn = {978-1-937284-20-6},
journal = {2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {142--151},
title = {{Structured perceptron with inexact search}},
url = {http://dl.acm.org/citation.cfm?id=2382029.2382049},
year = {2012}
}
@article{Yang2017,
abstract = {Content-dense news report important factual information about an event in direct, succinct manner. Information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. Here we empirically test this assumption on news articles from the business, U.S. international relations, sports and science journalism domains. Our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. We heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. On manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. Our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. Domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. Domain independent classifiers reproduce better naive crowdsourced judgements. Classification prediction is high across all conditions, around 80{\%}.},
archivePrefix = {arXiv},
arxivId = {1704.00440},
author = {Yang, Yinfei and Nenkova, Ani},
doi = {10.1613/jair.5418},
eprint = {1704.00440},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yang, Nenkova - 2017 - Combining lexical and syntactic features for detecting content-dense texts in news.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {179--219},
title = {{Combining lexical and syntactic features for detecting content-dense texts in news}},
volume = {60},
year = {2017}
}
@misc{DeClercq2015,
author = {{De Clercq}, Orph{\'{e}}e and {Van de Kauter}, Marjan and Lefever, Els and Hoste, Veronique},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/De Clercq et al. - 2015 - Applying hybrid terminology extraction to aspect-based sentiment analysis.pdf:pdf},
isbn = {978-1-941643-40-2},
language = {eng},
publisher = {Association for Computational Linguistics 2015},
title = {{Applying hybrid terminology extraction to aspect-based sentiment analysis}},
url = {http://lib.ugent.be/catalog/pug01:5841585},
year = {2015}
}
@article{Grishman2001,
abstract = {In this study of sublanguage, two issues are of particular interest to us as developers of adaptive IE systems: discovery methods and information formatting (structuring).},
author = {Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Grishman - 2001 - Adaptive information extraction and sublanguage analysis.pdf:pdf},
journal = {Proc. of IJCAI 2001},
pages = {1--4},
title = {{Adaptive information extraction and sublanguage analysis}},
url = {http://nlp.cs.nyu.edu/publication/papers/grishman-ijcai01.pdf},
year = {2001}
}
@article{Huttunen2002,
abstract = {This paper presents new Information Extraction scenarios which are linguistically and structurally more challenging than the traditional MUC scenarios. Traditional views on event structure and template design are not adequate for the more complex scenarios.The focus of this paper is to show the complexity of the scenarios, and propose a way to recover the structure of the event. First we identify two structural factors that contribute to the complexity of scenarios: the scattering of events in text, and inclusion relationships between events. These factors cause difficulty in representing the facts in an unambiguous way. Then we propose a modular, hierarchical representation where the information is split in atomic units represented by templates, and where the inclusion relationships between the units are indicated by links. Lastly, we discuss how we may recover this representation from text, with the help of linguistic cues linking the events.},
author = {Huttunen, Silja and Yangarber, Roman and Grishman, Ralph},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Huttunen, Yangarber, Grishman - 2002 - Complexity of event structure in IE scenarios.pdf:pdf},
journal = {Proc. of COLING},
title = {{Complexity of event structure in IE scenarios}},
url = {papers2://publication/uuid/915ECB2C-F088-40CD-9475-22EB8C654FFC},
year = {2002}
}
@article{Maeda2006,
author = {Maeda, K. and Lee, H. and Medero, J. and Strassel, S.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Maeda et al. - 2006 - A New Phase in Annotation Tool Development at the Linguistic Data Consortium The Evolution of the Annotation Graph.pdf:pdf},
journal = {Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06)},
pages = {1570--1573},
title = {{A New Phase in Annotation Tool Development at the Linguistic Data Consortium: The Evolution of the Annotation Graph Toolkit}},
url = {http://aclweb.org/anthology/L06-1488},
year = {2006}
}
@article{Li2013,
abstract = {Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe- art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.},
author = {Li, Qi and Ji, Heng and Huang, Liang},
doi = {10.1021/bi00231a020},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li, Ji, Huang - 2013 - Joint Event Extraction via Structured Prediction with Global Features.pdf:pdf},
isbn = {9781937284503},
issn = {15204995},
journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {73--82},
pmid = {2021618},
title = {{Joint Event Extraction via Structured Prediction with Global Features}},
url = {http://www.aclweb.org/anthology/P13-1008},
year = {2013}
}
@article{Haghighi2010,
abstract = {Abstract Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner. Our ...$\backslash$n},
author = {Haghighi, A and Klein, D},
doi = {10.3115/1608810.1608821},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Haghighi, Klein - 2010 - Coreference resolution in a modular, entity-centered model.pdf:pdf},
isbn = {1932432655},
journal = {{\ldots} Technologies: The 2010 Annual Conference of {\ldots}},
number = {June},
pages = {385--393},
title = {{Coreference resolution in a modular, entity-centered model}},
url = {http://dl.acm.org/citation.cfm?id=1858060{\%}5Cnpapers3://publication/uuid/D4326918-0F0B-4C43-88EF-846472B2D2B5},
year = {2010}
}
@misc{Consortium2008,
abstract = {The Entity Detection task requires that selected types of entities mentioned in the source data be detected, their sense disambiguated, and that selected attributes of these entities be extracted and merged into a unified representation for each entity.},
booktitle = {Facilities},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Unknown - 2008 - ACE English Annotation Guidelines for Entities (v6.6).pdf:pdf},
pages = {72},
publisher = {Linguistics Data Consortium},
title = {{ACE English Annotation Guidelines for Entities (v6.6)}},
url = {http://projects.ldc.upenn.edu/ace/docs/English-Entities-Guidelines{\_}v6.6.pdf},
year = {2008}
}
@article{Yang2016,
abstract = {Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.},
archivePrefix = {arXiv},
arxivId = {1609.03632},
author = {Yang, Bishan and Mitchell, Tom},
doi = {10.18653/v1/N16-1033},
eprint = {1609.03632},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yang, Mitchell - 2016 - Joint Extraction of Events and Entities within a Document Context(2).pdf:pdf;:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Yang, Mitchell - 2016 - Joint Extraction of Events and Entities within a Document Context.pdf:pdf},
isbn = {9781941643914},
pages = {289--299},
title = {{Joint Extraction of Events and Entities within a Document Context}},
url = {http://arxiv.org/abs/1609.03632},
year = {2016}
}
@article{Zavarella2014,
author = {Zavarella, Vanni and Dilek, K. and Ali, H.},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Zavarella, Dilek, Ali - 2014 - Event Extraction for Balkan Languages.pdf:pdf},
journal = {Eacl2014},
pages = {65--68},
title = {{Event Extraction for Balkan Languages}},
year = {2014}
}
@article{Rahman2011,
abstract = {While world knowledge has been shown to improve learning-based coreference resolvers, the improvements were typically obtained by incorporating world knowledge into a fairly weak baseline resolver. Hence, it is not clear whether these benefits can carry over to a stronger baseline. Moreover, since there has been no attempt to apply different sources of world knowledge in combination to corefer- ence resolution, it is not clear whether they of- fer complementary benefits to a resolver. We systematically compare commonly-used and under-investigated sources of world knowl- edge for coreference resolution by applying them to two learning-based coreference mod- els and evaluating them on documents anno- tated with two different annotation schemes.},
author = {Rahman, Altaf and Ng, Vincent},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Rahman, Ng - 2011 - Coreference resolution with world knowledge.pdf:pdf},
isbn = {1932432876},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1},
pages = {814--824},
title = {{Coreference resolution with world knowledge}},
year = {2011}
}
@article{Ji2009,
abstract = {This paper proposes a new task of cross-document event extraction and tracking and its evaluation metrics. We identify important person entities which are frequently involved in events as 'centroid entities'. Then we link the events involving the same centroid entity along a time line. We also present a system performing this task and our current approaches to address the main research challenges. We demonstrate that global inference from background knowledge and cross-document event aggregation are crucial to enhance the performance. This new task defines several extensions to the traditional single-document Information Extraction paradigm beyond 'slot filling'.},
author = {Ji, Heng and Grishman, Ralph and Chen, Zheng and Gupta, Prashant},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ji et al. - 2009 - Cross-document Event Extraction and Tracking Task, Evaluation, Techniques and Challenges.pdf:pdf},
issn = {13138502},
journal = {Proc. Recent Advances in Natural Language Processing 2009},
keywords = {cross-document extraction,event,information extraction},
pages = {166--172},
title = {{Cross-document Event Extraction and Tracking: Task, Evaluation, Techniques and Challenges}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.158.7055{\%}5Cnhttp://www.aclweb.org/anthology/R/R09/R09-1032.pdf},
year = {2009}
}
@article{Lee2011,
abstract = {This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task. Our system is a collection of deterministic coreference resolution mod- els that incorporate lexical, syntactic, seman- tic, and discourse information. All these mod- els use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both pre- dicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.},
author = {Lee, Heeyoung and Peirsman, Yves and Chang, Angel and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lee et al. - 2011 - Stanford ' s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task.pdf:pdf},
isbn = {9781937284084},
journal = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics},
pages = {28--34},
title = {{Stanford ' s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task}},
year = {2011}
}
@book{Chen2008,
author = {Chen, H. and Yang, C. C.},
isbn = {9783540692072},
publisher = {Springer Berlin Heidelberg},
series = {Studies in Computational Intelligence},
title = {{Intelligence and Security Informatics: Techniques and Applications}},
url = {https://books.google.be/books?id=LJ0R8wprgGgC},
year = {2008}
}
@article{Liu2014,
abstract = {Event coreference is an important task for full text analysis. However, previous work uses a variety of approaches, sources and evaluation, making the literature confusing and the results incommensurate. We provide a description of the differences to facilitate future research. Second, we present a supervised method for event coreference resolution that uses a rich feature set and propagates information alternatively between events and their arguments, adapting appropriately for each type of argument.},
author = {Liu, Zhengzhong and Araki, Jun and Hovy, Eduard and Mitamura, Teruko},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Liu et al. - 2014 - Supervised Within-Document Event Coreference using Information Propagation.pdf:pdf},
isbn = {978-2-9517408-8-4},
journal = {Proceedings of the 9th Edition of the Language, Resources and Evaluation Conference (LREC 2014)},
keywords = {event coreference,event coreference evaluation,information extraction},
pages = {4539--4544},
title = {{Supervised Within-Document Event Coreference using Information Propagation}},
year = {2014}
}
@article{Feng2016,
abstract = {{\textcopyright} 2016 Association for Computational Linguistics. Event detection remains a challenge due to the difficulty at encoding the word semantics in various contexts. Previous approaches heavily depend on languagespecific knowledge and pre-existing natural language processing (NLP) tools. However, compared to English, not all languages have such resources and tools available. A more promising approach is to automatically learn effective features from data, without relying on languagespecific resources. In this paper, we develop a hybrid neural network to capture both sequence and chunk information from specific contexts, and use them to train an event detector for multiple languages without any manually encoded features. Experiments show that our approach can achieve robust, efficient and accurate results for multiple languages (English, Chinese and Spanish).},
author = {Feng, X. and Huang, Liang and Tang, Duyu and Qin, B. and Ji, Heng and Liu, Ting},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Feng et al. - 2016 - A language-independent neural network for event detection.pdf:pdf},
isbn = {9781510827592},
journal = {54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Short Papers},
title = {{A language-independent neural network for event detection}},
year = {2016}
}
@article{Sil2016,
abstract = {Entity linking (EL) is the task of dis-ambiguating mentions in text by associ-ating them with entries in a predefined database of mentions (persons, organiza-tions, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other lan-guages, such as Spanish or Chinese. In this paper, we introduce LIEL, a Lan-guage Independent Entity Linking system, which provides an EL framework which, once trained on one language, works re-markably well on a number of different languages without change. LIEL makes a joint global prediction over the entire document, employing a discriminative re-ranking framework with many domain and language-independent feature func-tions. Experiments on numerous bench-mark datasets, show that the proposed sys-tem, once trained on one language, En-glish, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competi-tor system), demonstrating the viability of the approach.},
archivePrefix = {arXiv},
arxivId = {1712.01797},
author = {Sil, Avirup and Florian, Radu},
eprint = {1712.01797},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Sil, Florian - 2016 - One for All Towards Language Independent Named Entity Linking.pdf:pdf},
isbn = {9781510827585},
journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)},
pages = {2255--2264},
title = {{One for All : Towards Language Independent Named Entity Linking}},
year = {2016}
}
@article{Fabbri,
author = {Fabbri, Smit and Kova{\v{c}}, Herrero and Kovvv, Kowal and {\"{O}}sterreich, Deutschland and Lef{\`{e}}vre, Belgien and Schmidt, Frankreich and Smed, Schmiedl and Smith, {\v{S}}mit Ungarn},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Fabbri et al. - Unknown - Europa baut auf Biographien Aspekte, Bausteine, Normen und Standards f{\"{u}}r eine europ{\"{a}}ische Biographik.pdf:pdf},
title = {{Europa baut auf Biographien Aspekte, Bausteine, Normen und Standards f{\"{u}}r eine europ{\"{a}}ische Biographik}}
}
@article{Li2016,
abstract = {The goal of Event extraction is to extract struc-tured information of events that are of interest from unstructured documents. Existing event extractors for social media suffer from two major problems: lack of context and informal nature. In this paper, instead of conducting event extraction solely on each social media message, we incorporate cross-genre knowl-edge to boost the event extractor performance. Experiment results demonstrate that without any additional annotations, our proposed ap-proach is able to provide 15{\%} absolute F-score improvement over the state-of-the-art.},
author = {Li, Hao and Ji, Heng},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Li, Ji - 2016 - Cross-genre Event Extraction with Knowledge Enrichment.pdf:pdf},
isbn = {9781941643914},
journal = {Naacl-Hlt2016},
pages = {1158--1162},
title = {{Cross-genre Event Extraction with Knowledge Enrichment}},
url = {http://nlp.cs.rpi.edu/paper/crossgenreevent2016.pdf},
year = {2016}
}
@article{Vossen,
abstract = {In this paper, we present a new method to obtain large volumes of high-quality text corpora with event data for studying identity and reference relations. We report on the current methods to create event reference data by annotating texts and deriving the event data a posteriori. Our method starts from event registries in which event data is defined a priori. From this data, we extract so-called Microworlds of referential data with the Reference Texts that report on these events. This makes it possible to easily establish referential relations with high precision and at a large scale. In a pilot, we successfully obtained data from these resources with extreme ambiguity and variation, while maintaining the identity and reference relations and without having to annotate large quantities of texts word-by-word. The data from this pilot was annotated using an annotation tool created specifically in order to validate our method and to enrich the reference texts with event coreference annotations. This annotation process resulted in the Gun Violence Corpus, whose development process and outcome are described in this paper.},
author = {Vossen, Piek and Ilievski, Filip and Postma, Marten and Segers, Roxane},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Vossen et al. - 2018 - Don't Annotate, but Validate a Data-to-Text Method for Capturing Event Data.pdf:pdf},
keywords = {event coreference,structured data,text corpora},
title = {{Don't Annotate, but Validate: a Data-to-Text Method for Capturing Event Data}}
}
@inproceedings{BarbuMititelu,
abstract = {Preface This eighth meeting of the international Wordnet community coincides with the 15 th anniversary of the Global WordNet Association and the 30 th anniversary of the Princeton WordNet. We are delighted to welcome old and new colleagues from many countries and four continents who construct wordnets, ontologies and related tools, as well as colleagues who apply such resources in a wide range of Natural Language Applications or pursue research in lexical semantics. The number of wordnets has risen to over 150 and includes – besides all the major world languages – many less-studied languages such as Albanian and Nepali. Wordnets have become a principal tool in computational linguistics and NLP, and wordnet, SemCor and synset have entered the language as common nouns. Coming together and sharing some of the results of our work is an important part of the larger collaborative effort to better understand both universal and particular properties of human languages. Many people have donated their time and effort to make this meeting possible: the review committee, the local organizers and their helpers (Eric Curea, Maria Mitrofan, Elena Irimia), our sponsors (PIM, QATAR Airways, Oxford University Press), EasyChair and our host, the Romanian Academy. Above all, thanks go to you, the contributors, for traveling to Bucharest to present your work, listen and discuss.},
address = {Bucharest},
author = {{Barbu Mititelu}, Verginica and Forăscu, Corina and Fellbaum, Christiane and Vossen, Piek},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Barbu Mititelu et al. - 2016 - Proceedings of the Eighth Global WordNet Conference.pdf:pdf},
isbn = {978-973-0-20728-6},
pages = {448},
title = {{Proceedings of the Eighth Global WordNet Conference}},
year = {2016}
}
@article{Mostafazadeh2016,
abstract = {Learning commonsense causal and temporal relation between events is one of the major steps towards deeper language understanding. This is even more crucial for understanding stories and script learning. A prerequisite for learning scripts is a semantic framework which enables capturing rich event structures. In this paper we introduce a novel semantic annotation framework, called Causal and Temporal Relation Scheme (CaTeRS), which is unique in simultaneously capturing a comprehensive set of temporal and causal relations between events. By annotating a total of 1,600 sentences in the context of 320 five-sentence short stories sampled from ROCStories corpus, we demonstrate that these stories are indeed full of causal and temporal relations. Furthermore, we show that the CaTeRS annotation scheme enables high inter-annotator agreement for broad-coverage event entity annotation and moderate agreement on semantic link annotation},
author = {Mostafazadeh, Nasrin and Grealish, Alyson and Chambers, Nathanael and Allen, James and Vanderwende, Lucy},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Mostafazadeh et al. - 2016 - CaTeRS Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures.pdf:pdf},
journal = {Working paper},
pages = {51--61},
title = {{CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures}},
year = {2016}
}
@article{Ponzetto2006,
abstract = {In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.},
author = {Ponzetto, S P and Ponzetto, S P and Strube, M and Strube, M},
doi = {10.3115/1220835.1220860},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ponzetto et al. - 2006 - Exploiting semantic role lebeling, WordNet and Wikipedia for coreference resolution.pdf:pdf},
journal = {proceedings of NAACL 2006},
number = {June},
pages = {192--199},
pmid = {2},
title = {{Exploiting semantic role lebeling, WordNet and Wikipedia for coreference resolution.}},
volume = {33},
year = {2006}
}
@article{Ellis2015,
abstract = {Knowledge Base Population (KBP) is an evaluation track of the Text Analysis Conference (TAC), a workshop series organized by the National Institute of Standards and Technology (NIST). In 2015, TAC KBP's seventh year of operation, the evaluations focused on four tracks targeting information extraction and question answering technologies: Tri-lingual Entity Discovery {\&} Linking, Cold Start, Event Argument Linking, and Event Nuggets. Linguistic Data Consortium (LDC) at the University of Pennsylvania has supported TAC KBP since 2009 and continued again in 2015, developing, maintaining, and distributing new and existing linguistic resources for the evaluation series, including queries, human-generated responses, assessments, and tools and specifications. This paper describes LDC's resource creation efforts and their results in support of TAC KBP 2015.},
author = {Ellis, Joe and Getman, Jeremy and Fore, Dana and Kuster, Neil and Song, Zhiyi and Bies, Ann and Strassel, Stephanie},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ellis et al. - 2015 - Overview of Linguistic Resources for the TAC KBP 2015 Evaluations Methodologies and Results(2).pdf:pdf;:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ellis et al. - 2015 - Overview of Linguistic Resources for the TAC KBP 2015 Evaluations Methodologies and Results.pdf:pdf;:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ellis et al. - 2015 - Overview of Linguistic Resources for the TAC KBP 2015 Evaluations Methodologies and Results(3).pdf:pdf},
journal = {Proceedings of TAC KBP 2015 Workshop, National Institute of Standards and Technology, Maryland, USA},
number = {Ldc},
title = {{Overview of Linguistic Resources for the TAC KBP 2015 Evaluations : Methodologies and Results}},
year = {2015}
}
@article{Patwardhan2009,
abstract = {Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.},
author = {Patwardhan, Siddharth and Riloff, Ellen},
doi = {10.3115/1699510.1699530},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Patwardhan, Riloff - 2009 - A unified model of phrasal and sentential evidence for information extraction.pdf:pdf},
isbn = {9781932432596},
issn = {1932432590},
journal = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 1 - EMNLP '09},
pages = {151},
title = {{A unified model of phrasal and sentential evidence for information extraction}},
url = {http://portal.acm.org/citation.cfm?doid=1699510.1699530},
volume = {1},
year = {2009}
}
@article{Rospocher2016,
abstract = {Knowledge graphs have gained increasing popularity in the past couple of years, thanks to their adoption in everyday search engines. Typically, they consist of fairly static and encyclopedic facts about persons and organizations-e.g. a celebrity's birth date, occupation and family members-obtained from large repositories such as Freebase or Wikipedia. In this paper, we present a method and tools to automatically build knowledge graphs from news articles. As news articles describe changes in the world through the events they report, we present an approach to create Event-Centric Knowledge Graphs (ECKGs) using state-of-the-art natural language processing and semantic web techniques. Such ECKGs capture long-term developments and histories on hundreds of thousands of entities and are complementary to the static encyclopedic information in traditional knowledge graphs. We describe our event-centric representation schema, the challenges in extracting event information from news, our open source pipeline, and the knowledge graphs we have extracted from four different news corpora: general news (Wikinews), the FIFA world cup, the Global Automotive Industry, and Airbus A380 airplanes. Furthermore, we present an assessment on the accuracy of the pipeline in extracting the triples of the knowledge graphs. Moreover, through an event-centered browser and visualization tool we show how approaching information from news in an event-centric manner can increase the user's understanding of the domain, facilitates the reconstruction of news story lines, and enable to perform exploratory investigation of news hidden facts.},
author = {Rospocher, Marco and {Van Erp}, Marieke and Vossen, Piek and Fokkens, Antske and Aldabe, Itziar and Rigau, German and Soroa, Aitor and Ploeger, Thomas and Bogaard, Tessel},
doi = {10.1016/j.websem.2015.12.004},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Rospocher et al. - 2016 - Building event-centric knowledge graphs from news.pdf:pdf},
isbn = {http://dx.doi.org/10.1016/j.websem.2015.12.004},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Big data,Event extraction,Event-centric knowledge,Information integration,Natural language processing,Real world data},
title = {{Building event-centric knowledge graphs from news}},
year = {2016}
}
@article{VanEynde2004,
author = {{Van Eynde}, Frank},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Van Eynde - 2004 - Part of Speech Tagging en lemmatisering van het Corpus Gesproken Nederlands.pdf:pdf},
journal = {The Language and Speech Website},
pages = {87},
title = {{Part of Speech Tagging en lemmatisering van het Corpus Gesproken Nederlands}},
year = {2004}
}
@article{Lample2016,
abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.},
archivePrefix = {arXiv},
arxivId = {1603.01360},
author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
doi = {10.18653/v1/N16-1030},
eprint = {1603.01360},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Lample et al. - 2016 - Neural Architectures for Named Entity Recognition.pdf:pdf},
isbn = {9781941643914},
issn = {1045-9227},
pmid = {18244602},
title = {{Neural Architectures for Named Entity Recognition}},
url = {http://arxiv.org/abs/1603.01360},
year = {2016}
}
@article{Judea2015,
abstract = {Based on the hypothesis that frame-semantic parsing and event extraction are structurally identical tasks, we retrain SEMAFOR, a state- of-the-art frame-semantic parsing system to predict event triggers and arguments. We de- scribe how we change SEMAFOR to be better suited for the new task and show that it per- forms comparable to one of the best systems in event extraction. We also describe a bias in one of its models and propose a feature factor- ization which is better suited for this model. 1},
author = {Judea, Alex and Strube, Michael},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Judea, Strube - 2015 - Event Extraction as Frame-Semantic Parsing.pdf:pdf},
isbn = {9781941643396},
journal = {Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics},
pages = {159--164},
title = {{Event Extraction as Frame-Semantic Parsing}},
url = {http://www.cs.brown.edu/{~}dmcc/papers/dmcc-acl-2011.pdf},
year = {2015}
}
@article{Ahn2006,
abstract = {Event detection and recognition is a com-plex task consisting of multiple sub-tasks of varying difficulty. In this paper, we present a simple, modular approach to event extraction that allows us to exper-iment with a variety of machine learning methods for these sub-tasks, as well as to evaluate the impact on performance these sub-tasks have on the overall task.},
author = {Ahn, David},
doi = {10.3115/1629235.1629236},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Ahn - 2006 - The stages of event extraction.pdf:pdf},
isbn = {1932432817},
journal = {Proceedings of the Workshop on Annotating and Reasoning about Time and Events - ARTE '06},
number = {July},
pages = {1--8},
title = {{The stages of event extraction}},
url = {http://portal.acm.org/citation.cfm?doid=1629235.1629236},
year = {2006}
}
@misc{Colruyt2018,
author = {Colruyt, Camiel},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Colruyt - 2018 - Event extraction what is it and what's going on (1403 draft).pdf:pdf},
number = {March},
pages = {1--13},
title = {{Event extraction: what is it and what's going on (14/03 draft)}},
year = {2018}
}
@inproceedings{Song2015,
abstract = {We describe the evolution of the Entities, Re-lations and Events (ERE) annotation task, cre-ated to support research and technology development within the DARPA DEFT pro-gram. We begin by describing the specifica-tion for Light ERE annotation, including the motivation for the task within the context of DEFT. We discuss the transition from Light ERE to a more complex Rich ERE specifica-tion, enabling more comprehensive treatment of phenomena of interest to DEFT.},
address = {Denver, Colorado},
author = {Song, Zhiyi and Bies, Ann and Strassel, Stephanie and Riese, Tom and Mott, Justin and Ellis, Joe and Wright, Jonathan and Kulick, Seth and Ryant, Neville and Ma, Xiaoyi},
booktitle = {Proceedings of the 3rd Workshop on EVENTS at the NAACL-HLT 2015},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Song et al. - 2015 - From Light to Rich ERE Annotation of Entities, Relations, and Events.pdf:pdf},
pages = {89--98},
publisher = {ACL},
title = {{From Light to Rich ERE: Annotation of Entities, Relations, and Events}},
year = {2015}
}
@incollection{Vossen2018,
abstract = {In this paper, we present a new method to obtain large volumes of high-quality text corpora with event data for studying identity and reference relations. We report on the current methods to create event reference data by annotating texts and deriving the event data a posteriori. Our method starts from event registries in which event data is defined a priori. From this data, we extract so-called Microworlds of referential data with the Reference Texts that report on these events. This makes it possible to easily establish referential relations with high precision and at a large scale. In a pilot, we successfully obtained data from these resources with extreme ambiguity and variation, while maintaining the identity and reference relations and without having to annotate large quantities of texts word-by-word. The data from this pilot was annotated using an annotation tool created specifically in order to validate our method and to enrich the reference texts with event coreference annotations. This annotation process resulted in the Gun Violence Corpus, whose development process and outcome are described in this paper.},
author = {Vossen, Piek and Ilievski, Filip and Postma, Marten and Roxane, Segers},
booktitle = {LREC2018, Myazaki},
file = {:C$\backslash$:/Users/ccolruyt/Dropbox/{\#}NewsDNA/Literature/Event Extraction/camiel{\_}bibliography/Vossen et al. - 2018 - Don't Annotate, but Validate a Data-to-Text Method for Capturing Event Data.pdf:pdf},
keywords = {ulm1,ulm4},
title = {{Don't Annotate, but Validate: a Data-to-Text Method for Capturing Event Data}},
year = {2018}
}
